#!/usr/bin/env python3
"""
Note Scraper Auto - å®Œå…¨è‡ªå‹•ç‰ˆ
ãƒ­ã‚°ã‚¤ãƒ³ã®ã¿æ‰‹å‹•ã€ãã®ä»–ã¯å…¨è‡ªå‹•
"""

import asyncio
import sys
import os
from datetime import datetime
from typing import List, Dict, Optional
import pandas as pd
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Page, Browser
import argparse
import json
import re
import time
from urllib.parse import urljoin, urlparse


class NoteScraperAuto:
    """å®Œå…¨è‡ªå‹•ç‰ˆNoteã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.base_url = "https://note.com"
        
    async def initialize(self):
        """ãƒ–ãƒ©ã‚¦ã‚¶ã‚’åˆæœŸåŒ–"""
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(
            headless=self.headless,
            args=[
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-web-security',
                '--disable-dev-shm-usage',
                '--no-first-run',
                '--disable-background-timer-throttling'
            ]
        )
        context = await self.browser.new_context(
            viewport={'width': 1280, 'height': 720},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        )
        self.page = await context.new_page()
        
    async def close(self):
        """ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã‚‹"""
        if self.browser:
            await self.browser.close()
            
    async def wait_for_login_only(self, author_url: str):
        """ãƒ­ã‚°ã‚¤ãƒ³ã®ã¿æ‰‹å‹•å¾…æ©Ÿ"""
        print("\n" + "="*80)
        print("ğŸ” ãƒ­ã‚°ã‚¤ãƒ³ã®ã¿æ‰‹å‹•ãƒ¢ãƒ¼ãƒ‰")
        print("="*80)
        print("ğŸ“‹ æ‰‹é †:")
        print("1. ãƒ–ãƒ©ã‚¦ã‚¶ã§noteãŒé–‹ãã¾ã™")
        print("2. ã€é‡è¦ã€‘ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„")
        print("3. ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†å¾Œã€ä¸‹è¨˜ã®æ–¹æ³•ã§é€šçŸ¥ã—ã¦ãã ã•ã„")
        print("4. ãã®å¾Œã¯å®Œå…¨è‡ªå‹•ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¾ã™")
        print("="*80)
        
        # è¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸ã«ç§»å‹•
        article_list_url = author_url.rstrip('/') + '/all'
        
        try:
            await self.page.goto(article_list_url, wait_until="domcontentloaded", timeout=30000)
            await self.page.wait_for_timeout(3000)
            print(f"âœ… è¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’é–‹ãã¾ã—ãŸ: {article_list_url}")
            print("ğŸ‘† ãƒ–ãƒ©ã‚¦ã‚¶ã§ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„")
        except Exception as e:
            print(f"âŒ ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’å¾…æ©Ÿ
        print(f"\nğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†å¾Œã€ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®æ–¹æ³•ã§é€šçŸ¥ã—ã¦ãã ã•ã„:")
        print("   1. ã“ã®Claude Codeã§ã€Œãƒ­ã‚°ã‚¤ãƒ³å®Œäº†ã€ã¨å…¥åŠ›")
        print("   2. ã¾ãŸã¯ã€Macã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚¢ãƒ—ãƒªã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ:")
        print(f"      touch {os.getcwd()}/login_done.txt")
        
        try:
            input()
            print("âœ… ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†ã‚’ç¢ºèªã—ã¾ã—ãŸ")
        except EOFError:
            print("â³ ãƒ•ã‚¡ã‚¤ãƒ«å¾…æ©Ÿãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™...")
            login_file = os.path.join(os.getcwd(), "login_done.txt")
            print(f"ğŸ“ {login_file} ã®ä½œæˆã‚’å¾…æ©Ÿä¸­...")
            
            while not os.path.exists(login_file):
                time.sleep(1)
                print(".", end="", flush=True)
                
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            os.remove(login_file)
            print("\nâœ… ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†ã‚’ç¢ºèªã—ã¾ã—ãŸ")
            
        return True
        
    async def auto_load_all_articles(self):
        """ã€Œã‚‚ã£ã¨ã¿ã‚‹ã€ã‚’è‡ªå‹•ã‚¯ãƒªãƒƒã‚¯ã—ã¦å…¨è¨˜äº‹ã‚’èª­ã¿è¾¼ã¿"""
        print("ğŸ”„ å…¨è¨˜äº‹ã‚’è‡ªå‹•èª­ã¿è¾¼ã¿ä¸­...")
        max_attempts = 200  # æœ€å¤§200å›ã¾ã§
        previous_article_count = 0
        
        for attempt in range(max_attempts):
            # ç¾åœ¨ã®è¨˜äº‹æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            current_links = await self.page.query_selector_all('a[href*="/n/"]')
            current_count = len([link for link in current_links 
                               if await self.is_article_link(link)])
            
            print(f"  è¨˜äº‹æ•°: {current_count} (è©¦è¡Œ {attempt + 1})")
            
            # è¨˜äº‹æ•°ãŒå¢—ãˆãªããªã£ãŸã‚‰æ•°å›è©¦è¡Œã—ã¦çµ‚äº†
            if current_count == previous_article_count and attempt > 2:
                print(f"  âœ… å…¨è¨˜äº‹èª­ã¿è¾¼ã¿å®Œäº†: {current_count}è¨˜äº‹")
                break
                
            previous_article_count = current_count
            
            # ã€Œã‚‚ã£ã¨ã¿ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’æ¢ã—ã¦ã‚¯ãƒªãƒƒã‚¯ï¼ˆã²ã‚‰ãŒãªã‚‚å«ã‚€ï¼‰
            more_button_selectors = [
                'button:has-text("ã‚‚ã£ã¨è¦‹ã‚‹")',
                'button:has-text("ã‚‚ã£ã¨ã¿ã‚‹")',  # ã²ã‚‰ãŒãªè¿½åŠ 
                'button:has-text("ã•ã‚‰ã«èª­ã¿è¾¼ã‚€")',
                'button:has-text("ç¶šãã‚’èª­ã‚€")',
                '[data-testid="load-more"]',
                '.load-more',
                'button[aria-label="ã‚‚ã£ã¨è¦‹ã‚‹"]',
                'button[aria-label="ã‚‚ã£ã¨ã¿ã‚‹"]',  # ã²ã‚‰ãŒãªè¿½åŠ 
                'a:has-text("ã‚‚ã£ã¨è¦‹ã‚‹")',
                'a:has-text("ã‚‚ã£ã¨ã¿ã‚‹")'  # ã²ã‚‰ãŒãªè¿½åŠ 
            ]
            
            button_found = False
            for selector in more_button_selectors:
                try:
                    button = await self.page.query_selector(selector)
                    if button:
                        # ãƒœã‚¿ãƒ³ãŒè¦‹ãˆã‚‹ä½ç½®ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                        await button.scroll_into_view_if_needed()
                        await self.page.wait_for_timeout(1000)
                        
                        # ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
                        await button.click()
                        print(f"  ğŸ”„ ã€Œã‚‚ã£ã¨ã¿ã‚‹ã€ã‚’ã‚¯ãƒªãƒƒã‚¯")
                        button_found = True
                        
                        # èª­ã¿è¾¼ã¿å¾…æ©Ÿ
                        await self.page.wait_for_timeout(3000)
                        break
                        
                except Exception as e:
                    continue
            
            if not button_found:
                # ãƒœã‚¿ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã§è©¦è¡Œ
                await self.page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                await self.page.wait_for_timeout(2000)
                
                # ã•ã‚‰ã«å¾…æ©Ÿã—ã¦ç„¡é™ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’è©¦è¡Œ
                await self.page.wait_for_timeout(3000)
                
        print(f"ğŸ¯ æœ€çµ‚è¨˜äº‹æ•°: {previous_article_count}")
        
    async def is_article_link(self, link):
        """æœ‰åŠ¹ãªè¨˜äº‹ãƒªãƒ³ã‚¯ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            href = await link.get_attribute('href')
            if href and '/n/' in href and not href.endswith('/n/'):
                if re.match(r'.*/n/[a-zA-Z0-9_-]+', href):
                    return '/info/n/' not in href
        except:
            pass
        return False
        
    async def collect_all_articles(self) -> List[Dict]:
        """ç¾åœ¨è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹å…¨è¨˜äº‹ã‚’åé›†"""
        print("ğŸ“ è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹å…¨è¨˜äº‹ã‚’åé›†ä¸­...")
        
        articles = []
        
        # å°‘ã—å¾…æ©Ÿã—ã¦ã‹ã‚‰åé›†é–‹å§‹
        await self.page.wait_for_timeout(2000)
        
        # å…¨è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’å–å¾—
        all_links = await self.page.query_selector_all('a[href*="/n/"]')
        
        for link in all_links:
            try:
                if await self.is_article_link(link):
                    href = await link.get_attribute('href')
                    full_url = urljoin(self.base_url, href)
                    full_url = full_url.split('?')[0].split('#')[0]
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if not any(a['url'] == full_url for a in articles):
                        articles.append({'url': full_url})
            except Exception as e:
                continue
                
        print(f"âœ… {len(articles)} è¨˜äº‹ã‚’åé›†ã—ã¾ã—ãŸ")
        return articles
        
    async def scrape_article_content(self, url: str, index: int, total: int) -> Dict:
        """è¨˜äº‹å†…å®¹ã‚’å–å¾—"""
        print(f"ğŸ“„ è¨˜äº‹ {index}/{total}: {url}")
        
        try:
            await self.page.goto(url, wait_until="domcontentloaded", timeout=30000)
            await self.page.wait_for_timeout(2000)
            
            # HTMLã‚’å–å¾—ã—ã¦è§£æ
            content = await self.page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            data = {
                'url': url,
                'ã‚¿ã‚¤ãƒˆãƒ«': '',
                'æœ¬æ–‡': '',
                'å…¬é–‹æ—¥': '',
                'ä¾¡æ ¼': 'N/A',
                'è³¼å…¥çŠ¶æ³': 'è³¼å…¥æ¸ˆã¿ or ç„¡æ–™'
            }
            
            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ï¼‰
            page_title = await self.page.title()
            if page_title and 'ï½œnote' in page_title:
                data['ã‚¿ã‚¤ãƒˆãƒ«'] = page_title.split('ï½œnote')[0].strip()
            elif page_title:
                data['ã‚¿ã‚¤ãƒˆãƒ«'] = page_title
                
            # å…¬é–‹æ—¥å–å¾—
            time_elem = soup.select_one('time')
            if time_elem:
                data['å…¬é–‹æ—¥'] = time_elem.get('datetime', time_elem.get_text(strip=True))
                
            # æœ¬æ–‡å–å¾—
            content_selectors = [
                '.note-common-styles__textnote-body',
                '.p-article__content', 
                'article .content',
                '.note-body',
                '[data-testid="article-body"]'
            ]
            
            content_text = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content_text = content_elem.get_text(separator='\n', strip=True)
                    break
                    
            if not content_text:
                # å…¨ã¦ã®pè¦ç´ ã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡º
                paragraphs = soup.find_all('p')
                content_text = '\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
                
            data['æœ¬æ–‡'] = content_text
            
            # æœ‰æ–™è¨˜äº‹ãƒã‚§ãƒƒã‚¯
            if any(phrase in content for phrase in ['æœ‰æ–™', 'ã“ã®è¨˜äº‹ã¯æœ‰æ–™', 'ç¶šãã‚’ã¿ã‚‹ã«ã¯']):
                data['ä¾¡æ ¼'] = 'æœ‰æ–™'
                if 'ã“ã®ç¶šãã‚’ã¿ã‚‹ã«ã¯' in content or 'ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ç¶šãã‚’èª­ã‚€' in content:
                    data['è³¼å…¥çŠ¶æ³'] = 'æœªè³¼å…¥'
                    
            print(f"âœ… '{data['ã‚¿ã‚¤ãƒˆãƒ«'][:30]}...' ã‚’å–å¾—å®Œäº†")
            return data
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {
                'url': url,
                'ã‚¿ã‚¤ãƒˆãƒ«': f'ã‚¨ãƒ©ãƒ¼: {url}',
                'æœ¬æ–‡': f'å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}',
                'å…¬é–‹æ—¥': '',
                'ä¾¡æ ¼': 'N/A',
                'è³¼å…¥çŠ¶æ³': 'ã‚¨ãƒ©ãƒ¼'
            }
            
    async def run_scraping(self, author_url: str):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ"""
        await self.initialize()
        
        try:
            # ãƒ­ã‚°ã‚¤ãƒ³ã®ã¿æ‰‹å‹•å¾…æ©Ÿ
            login_success = await self.wait_for_login_only(author_url)
            if not login_success:
                print("âŒ ãƒ­ã‚°ã‚¤ãƒ³ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
                return
                
            # ã€Œã‚‚ã£ã¨ã¿ã‚‹ã€ã‚’è‡ªå‹•ã‚¯ãƒªãƒƒã‚¯ã—ã¦å…¨è¨˜äº‹èª­ã¿è¾¼ã¿
            await self.auto_load_all_articles()
            
            # è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹å…¨è¨˜äº‹ã‚’åé›†
            articles = await self.collect_all_articles()
            
            if not articles:
                print("âŒ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return
                
            print(f"\nğŸš€ {len(articles)} è¨˜äº‹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™")
            
            # å„è¨˜äº‹ã®å†…å®¹ã‚’å–å¾—
            results = []
            for i, article in enumerate(articles, 1):
                result = await self.scrape_article_content(article['url'], i, len(articles))
                results.append(result)
                
                # 10è¨˜äº‹ã”ã¨ã«é€²æ—è¡¨ç¤º
                if i % 10 == 0:
                    print(f"ğŸ’¾ é€²æ—: {i}/{len(articles)} è¨˜äº‹å®Œäº†")
                    
                await asyncio.sleep(1.5)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
                
            # CSVã«ä¿å­˜
            df = pd.DataFrame(results)
            df.insert(0, 'ç•ªå·', range(1, len(df) + 1))
            
            # ã‚«ãƒ©ãƒ é †åºã‚’èª¿æ•´
            columns = ['ç•ªå·', 'å…¬é–‹æ—¥', 'ã‚¿ã‚¤ãƒˆãƒ«', 'æœ¬æ–‡', 'ä¾¡æ ¼', 'è³¼å…¥çŠ¶æ³']
            df = df[columns]
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M")
            filename = f"ihayato_auto_{timestamp}.csv"
            filepath = os.path.join(os.getcwd(), filename)
            
            # CSVä¿å­˜
            df.to_csv(filepath, index=False, encoding='utf-8-sig')
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®—
            file_size = os.path.getsize(filepath) / 1024  # KB
            
            print(f"\nğŸ‰ å®Œäº†! {len(results)} è¨˜äº‹ã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸ")
            print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:.1f} KB")
            print(f"ğŸ“‚ ä¿å­˜å…ˆ: {filepath}")
            
        finally:
            await self.close()


async def main():
    parser = argparse.ArgumentParser(description='Note Scraper Auto')
    parser.add_argument('url', help='è‘—è€…ã®note URL')
    parser.add_argument('--headless', action='store_true', help='ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ')
    
    args = parser.parse_args()
    
    print("ğŸš€ Note Scraper Auto ã‚’é–‹å§‹")
    print(f"ğŸ“ å¯¾è±¡: {args.url}")
    
    scraper = NoteScraperAuto(headless=args.headless)
    await scraper.run_scraping(args.url)
    
    print("\nâœ… å‡¦ç†å®Œäº†!")


if __name__ == "__main__":
    asyncio.run(main())