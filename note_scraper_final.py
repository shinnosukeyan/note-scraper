#!/usr/bin/env python3
"""
Note Scraper Final - å®Œå…¨ç‰ˆ
ã‚¤ã‚±ãƒãƒ¤ã•ã‚“ã®Noteè¨˜äº‹ã‚’600è¨˜äº‹ä»¥ä¸Šå–å¾—
å®Œæˆãƒ‡ãƒ¼ã‚¿ã®å“è³ªã§æœ¬æ–‡ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å®Ÿè£…
"""

import asyncio
import sys
import os
from datetime import datetime
from typing import List, Dict, Optional
import pandas as pd
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Page, Browser
import re
import time
from urllib.parse import urljoin, urlparse


class NoteScraperFinal:
    """å®Œå…¨ç‰ˆNoteã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        self.headless = False
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.base_url = "https://note.com"
        self.articles: List[Dict] = []
        
    async def initialize(self):
        """ãƒ–ãƒ©ã‚¦ã‚¶ã‚’åˆæœŸåŒ–ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå®Œå…¨ç„¡åŠ¹åŒ–ï¼‰"""
        print("ğŸš€ Note Scraper Final ã‚’åˆæœŸåŒ–ä¸­...")
        
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(
            headless=self.headless,
            args=[
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-web-security',
                '--disable-dev-shm-usage',
                '--no-first-run'
            ]
        )
        
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå®Œå…¨ç„¡åŠ¹åŒ–
        context = await self.browser.new_context(
            viewport={'width': 1280, 'height': 720},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        )
        context.set_default_timeout(0)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç„¡ã—
        
        self.page = await context.new_page()
        self.page.set_default_timeout(0)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç„¡ã—
        
        print("âœ… ãƒ–ãƒ©ã‚¦ã‚¶åˆæœŸåŒ–å®Œäº†ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç„¡åŠ¹åŒ–æ¸ˆã¿ï¼‰")
    
    async def manual_setup(self, profile_url: str):
        """æ‰‹å‹•æº–å‚™ãƒ•ã‚§ãƒ¼ã‚º"""
        print("\n" + "="*70)
        print("ğŸ”§ æ‰‹å‹•æº–å‚™ãƒ•ã‚§ãƒ¼ã‚ºé–‹å§‹")
        print("="*70)
        
        # è¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸ã«ç§»å‹•
        article_list_url = profile_url.rstrip('/') + '/all'
        print(f"ğŸ“„ è¨˜äº‹ä¸€è¦§ã«ç§»å‹•: {article_list_url}")
        
        await self.page.goto(article_list_url, wait_until="domcontentloaded", timeout=0)
        await self.page.wait_for_timeout(3000)
        
        print("\nğŸ“‹ æ‰‹å‹•ä½œæ¥­ã®æ‰‹é †:")
        print("1. ğŸ“ ãƒ–ãƒ©ã‚¦ã‚¶ã§ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„")
        print("2. ğŸ”„ ã€Œã‚‚ã£ã¨ã¿ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’å…¨éƒ¨ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„")
        print("   ï¼ˆ600è¨˜äº‹ä»¥ä¸ŠãŒå…¨ã¦è¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§ï¼‰")
        print("3. âœ… å®Œäº†ã—ãŸã‚‰ã€ã“ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§Enterã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ãã ã•ã„")
        print()
        print("âš ï¸  é‡è¦:")
        print("   - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¯è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("   - ãƒ–ãƒ©ã‚¦ã‚¶ã¯è‡ªå‹•ã§é–‰ã˜ã¾ã›ã‚“")
        print("   - æ™‚é–“ã‚’ã‹ã‘ã¦ã‚‚å¤§ä¸ˆå¤«ã§ã™")
        print()
        
        # å®Œäº†ã¾ã§å¾…æ©Ÿï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–æ–¹å¼ï¼‰
        setup_file = "/Users/yusukeohata/Desktop/development/note-scraper/setup_done.txt"
        
        print("ğŸ‘† å®Œäº†ã—ãŸã‚‰ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print(f"   touch {setup_file}")
        print()
        print("â° setup_done.txt ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã‚’å¾…æ©Ÿä¸­...")
        
        while True:
            if os.path.exists(setup_file):
                print("âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ã‚’ç¢ºèªã—ã¾ã—ãŸ")
                os.remove(setup_file)  # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                break
            
            await asyncio.sleep(3)  # 3ç§’ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
        
        print("âœ… æ‰‹å‹•æº–å‚™å®Œäº†ï¼è‡ªå‹•å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
    
    async def collect_articles(self) -> List[Dict]:
        """è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’åé›†"""
        print("\nğŸ“ è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’åé›†ä¸­...")
        
        articles = []
        
        # å°‘ã—å¾…æ©Ÿ
        await self.page.wait_for_timeout(2000)
        
        # è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’åé›†
        all_links = await self.page.query_selector_all('a[href*="/n/"]')
        
        for link in all_links:
            try:
                href = await link.get_attribute('href')
                if href and '/n/' in href and not href.endswith('/n/'):
                    if re.match(r'.*/n/[a-zA-Z0-9_-]+', href):
                        if '/info/n/' not in href:
                            full_url = urljoin(self.base_url, href)
                            full_url = full_url.split('?')[0].split('#')[0]
                            
                            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                            if not any(article['url'] == full_url for article in articles):
                                articles.append({
                                    'url': full_url,
                                    'title': '',
                                    'content': '',
                                    'date': '',
                                    'price': '',
                                    'purchase_status': ''
                                })
                                
            except Exception as e:
                continue
        
        print(f"âœ… {len(articles)} è¨˜äº‹ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
        return articles
    
    async def scrape_article(self, article: Dict, index: int, total: int) -> Dict:
        """è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆå®Œæˆãƒ‡ãƒ¼ã‚¿å“è³ªï¼‰"""
        url = article['url']
        
        try:
            print(f"ğŸ“„ è¨˜äº‹ {index}/{total}: {url}")
            
            # ãƒšãƒ¼ã‚¸ã«ç§»å‹•ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç„¡ã—ï¼‰
            await self.page.goto(url, wait_until="domcontentloaded", timeout=0)
            await self.page.wait_for_timeout(2000)
            
            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            page_title = await self.page.title()
            if page_title and 'ï½œnote' in page_title:
                article['title'] = page_title.split('ï½œnote')[0].strip()
            
            # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’å–å¾—
            content = await self.page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            # æœ¬æ–‡å–å¾—ï¼ˆå®Œæˆãƒ‡ãƒ¼ã‚¿å“è³ªï¼‰
            article['content'] = await self.extract_formatted_content(soup)
            
            # å…¬é–‹æ—¥å–å¾—
            date_element = soup.find('time')
            if date_element and date_element.get('datetime'):
                article['date'] = date_element['datetime']
            
            # ä¾¡æ ¼æƒ…å ±å–å¾—
            price_element = soup.find('span', string=re.compile(r'ï¿¥|å††'))
            if price_element:
                article['price'] = 'æœ‰æ–™'
                article['purchase_status'] = 'è³¼å…¥æ¸ˆã¿ or ç„¡æ–™'
            else:
                article['price'] = 'ç„¡æ–™'
                article['purchase_status'] = 'ç„¡æ–™'
            
            print(f"âœ… '{article['title'][:50]}...' ã‚’å–å¾—å®Œäº†")
            
        except Exception as e:
            print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {url} - {e}")
            article['title'] = f"ã‚¨ãƒ©ãƒ¼: {e}"
            article['content'] = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
        
        return article
    
    async def extract_formatted_content(self, soup: BeautifulSoup) -> str:
        """å®Œæˆãƒ‡ãƒ¼ã‚¿å“è³ªã®æœ¬æ–‡ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæŠ½å‡º"""
        content_parts = []
        
        # ãƒ¡ã‚¤ãƒ³ã®è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
        article_body = soup.find('div', class_='note-common-styles__textnote-body')
        if not article_body:
            article_body = soup.find('div', class_='note-common-styles__textnote-body-container')
        
        if article_body:
            content_parts = self.process_content_elements(article_body)
        
        # æ”¹è¡Œã‚’ã€Œâ†’ã€ã§çµåˆ
        return '\n'.join(content_parts)
    
    def process_content_elements(self, element) -> List[str]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è¦ç´ ã‚’å‡¦ç†ã—ã¦å®Œæˆãƒ‡ãƒ¼ã‚¿å½¢å¼ã«å¤‰æ›"""
        parts = []
        
        for child in element.children:
            if child.name is None:  # ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰
                text = child.strip()
                if text:
                    parts.append(text)
            
            elif child.name == 'p':
                # ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•
                p_content = self.process_paragraph(child)
                if p_content:
                    parts.append(p_content)
                    parts.append('â†’')  # æ”¹è¡Œ
            
            elif child.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                # è¦‹å‡ºã—
                heading_text = child.get_text(strip=True)
                if heading_text:
                    parts.append(f"**{heading_text}**")
                    parts.append('â†’')
            
            elif child.name == 'blockquote':
                # å¼•ç”¨
                quote_text = child.get_text(strip=True)
                if quote_text:
                    parts.append(f"> {quote_text}")
                    parts.append('â†’')
            
            elif child.name == 'hr':
                # åŒºåˆ‡ã‚Šç·š
                parts.append('====')
                parts.append('â†’')
            
            elif child.name == 'img':
                # ç”»åƒ
                img_src = child.get('src', '')
                img_alt = child.get('alt', 'ç”»åƒ')
                if img_src:
                    parts.append(f"![{img_alt}]({img_src})")
                    parts.append('â†’')
            
            elif child.name == 'figure':
                # å›³è¡¨ï¼ˆç”»åƒå«ã‚€ï¼‰
                img = child.find('img')
                figcaption = child.find('figcaption')
                
                if img:
                    img_src = img.get('src', '')
                    img_alt = img.get('alt', 'ç”»åƒ')
                    if img_src:
                        parts.append(f"![{img_alt}]({img_src})")
                        if figcaption:
                            caption_text = figcaption.get_text(strip=True)
                            parts.append(f"*{caption_text}*")
                        parts.append('â†’')
            
            elif child.name == 'div':
                # åŸ‹ã‚è¾¼ã¿ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
                embed_content = self.process_embed_content(child)
                if embed_content:
                    parts.append(embed_content)
                    parts.append('â†’')
            
            elif child.name == 'ul' or child.name == 'ol':
                # ãƒªã‚¹ãƒˆ
                list_items = child.find_all('li')
                for li in list_items:
                    li_text = li.get_text(strip=True)
                    if li_text:
                        prefix = '- ' if child.name == 'ul' else '1. '
                        parts.append(f"{prefix}{li_text}")
                parts.append('â†’')
        
        return parts
    
    def process_paragraph(self, p_element) -> str:
        """ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã‚’å‡¦ç†"""
        text_parts = []
        
        for child in p_element.children:
            if child.name is None:  # ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰
                text = child.strip()
                if text:
                    text_parts.append(text)
            
            elif child.name == 'strong' or child.name == 'b':
                # å¤ªå­—
                strong_text = child.get_text(strip=True)
                if strong_text:
                    text_parts.append(f"**{strong_text}**")
            
            elif child.name == 'em' or child.name == 'i':
                # æ–œä½“
                em_text = child.get_text(strip=True)
                if em_text:
                    text_parts.append(f"*{em_text}*")
            
            elif child.name == 'a':
                # ãƒªãƒ³ã‚¯
                link_text = child.get_text(strip=True)
                link_href = child.get('href', '')
                if link_text and link_href:
                    text_parts.append(f"[{link_text}]({link_href})")
                elif link_href:
                    text_parts.append(f"[{link_href}]({link_href})")
            
            elif child.name == 'br':
                # æ”¹è¡Œ
                text_parts.append('â†’')
            
            else:
                # ãã®ä»–ã®è¦ç´ 
                other_text = child.get_text(strip=True)
                if other_text:
                    text_parts.append(other_text)
        
        return ''.join(text_parts)
    
    def process_embed_content(self, div_element) -> str:
        """åŸ‹ã‚è¾¼ã¿ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å‡¦ç†"""
        # åŸ‹ã‚è¾¼ã¿ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç‰¹å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        embed_link = div_element.find('a')
        if embed_link:
            href = embed_link.get('href', '')
            title_elem = embed_link.find('h3') or embed_link.find('h2') or embed_link.find('strong')
            desc_elem = embed_link.find('p')
            
            if href and title_elem:
                title = title_elem.get_text(strip=True)
                desc = desc_elem.get_text(strip=True) if desc_elem else ''
                domain = urlparse(href).netloc
                
                # å®Œæˆãƒ‡ãƒ¼ã‚¿å½¢å¼ã®åŸ‹ã‚è¾¼ã¿
                return f"[åŸ‹ã‚è¾¼ã¿ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: [**{title}***{desc}**{domain}*]({href})[{href}]({href})]({href})"
        
        return ''
    
    async def save_to_csv(self, articles: List[Dict], filename: str):
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        print(f"\nğŸ’¾ CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ä¸­: {filename}")
        
        # DataFrameã«å¤‰æ›
        df_data = []
        for i, article in enumerate(articles, 1):
            df_data.append({
                'ç•ªå·': i,
                'å…¬é–‹æ—¥': article['date'],
                'ã‚¿ã‚¤ãƒˆãƒ«': article['title'],
                'æœ¬æ–‡': article['content'],
                'ä¾¡æ ¼': article['price'],
                'è³¼å…¥çŠ¶æ³': article['purchase_status'],
                'URL': article['url']
            })
        
        df = pd.DataFrame(df_data)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
        file_size = os.path.getsize(filename)
        file_size_mb = file_size / (1024 * 1024)
        
        print(f"âœ… ä¿å­˜å®Œäº†!")
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {filename}")
        print(f"ğŸ“Š è¨˜äº‹æ•°: {len(articles)}")
        print(f"ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.1f} MB")
    
    async def run(self, profile_url: str):
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        try:
            print("ğŸš€ Note Scraper Final ã‚’é–‹å§‹")
            print(f"ğŸ“ å¯¾è±¡: {profile_url}")
            print()
            
            # åˆæœŸåŒ–
            await self.initialize()
            
            # æ‰‹å‹•æº–å‚™ãƒ•ã‚§ãƒ¼ã‚º
            await self.manual_setup(profile_url)
            
            # è¨˜äº‹åé›†
            articles = await self.collect_articles()
            
            if not articles:
                print("âŒ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return
            
            # å„è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            print(f"\nğŸ“„ {len(articles)} è¨˜äº‹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹...")
            
            for i, article in enumerate(articles, 1):
                await self.scrape_article(article, i, len(articles))
                await asyncio.sleep(1.5)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
            
            # CSVä¿å­˜
            timestamp = datetime.now().strftime("%Y%m%d_%H%M")
            filename = f"ihayato_final_{timestamp}.csv"
            await self.save_to_csv(articles, filename)
            
            print(f"\nğŸ‰ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†!")
            print(f"ğŸ“ æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«: {filename}")
            print(f"ğŸ“Š å–å¾—è¨˜äº‹æ•°: {len(articles)}")
            
        except KeyboardInterrupt:
            print("\nâ¸ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            if self.browser:
                await self.browser.close()


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    profile_url = "https://note.com/ihayato"
    
    scraper = NoteScraperFinal()
    await scraper.run(profile_url)


if __name__ == "__main__":
    asyncio.run(main())