#!/usr/bin/env python3
"""
Note Scraper Simple - ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ
ãƒ­ã‚°ã‚¤ãƒ³å¾Œã€Claude Codeã§ã€Œãƒ­ã‚°ã‚¤ãƒ³å®Œäº†ã€ã¨å…¥åŠ›ã™ã‚‹ã ã‘
"""

import asyncio
import sys
import os
from datetime import datetime
from typing import List, Dict, Optional
import pandas as pd
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Page, Browser
import argparse
import json
import re
import time
from urllib.parse import urljoin, urlparse


class NoteScraperSimple:
    """ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆNoteã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.base_url = "https://note.com"
        
    async def initialize(self):
        """ãƒ–ãƒ©ã‚¦ã‚¶ã‚’åˆæœŸåŒ–"""
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(
            headless=self.headless,
            args=['--no-sandbox', '--disable-setuid-sandbox']
        )
        context = await self.browser.new_context(
            viewport={'width': 1280, 'height': 720}
        )
        self.page = await context.new_page()
        
    async def close(self):
        """ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã‚‹"""
        if self.browser:
            await self.browser.close()
            
    async def wait_for_simple_login(self, author_url: str):
        """ã‚·ãƒ³ãƒ—ãƒ«ãƒ­ã‚°ã‚¤ãƒ³å¾…æ©Ÿ"""
        print("\n" + "="*60)
        print("ğŸ” ã‚·ãƒ³ãƒ—ãƒ«ãƒ­ã‚°ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰")
        print("="*60)
        print("1. ãƒ–ãƒ©ã‚¦ã‚¶ã§ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„")
        print("2. Claude Codeã§ã€Œãƒ­ã‚°ã‚¤ãƒ³å®Œäº†ã€ã¨å…¥åŠ›ã—ã¦ãã ã•ã„")
        print("="*60)
        
        # è¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸ã«ç§»å‹•
        article_list_url = author_url.rstrip('/') + '/all'
        
        try:
            await self.page.goto(article_list_url, wait_until="domcontentloaded")
            await self.page.wait_for_timeout(3000)
            print(f"âœ… ãƒšãƒ¼ã‚¸ã‚’é–‹ãã¾ã—ãŸ: {article_list_url}")
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            
        # ãƒ•ã‚¡ã‚¤ãƒ«å¾…æ©Ÿ
        login_file = os.path.join(os.getcwd(), "simple_login.txt")
        print(f"ğŸ“ å¾…æ©Ÿä¸­... simple_login.txt ã®ä½œæˆã‚’å¾…ã£ã¦ã„ã¾ã™")
        
        while True:
            if os.path.exists(login_file):
                break
            time.sleep(1)
            print(".", end="", flush=True)
            
        os.remove(login_file)
        print("\nâœ… ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†ï¼")
        
    async def auto_load_all_articles(self):
        """ã€Œã‚‚ã£ã¨ã¿ã‚‹ã€ã‚’è‡ªå‹•ã‚¯ãƒªãƒƒã‚¯"""
        print("ğŸ”„ å…¨è¨˜äº‹ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        
        for attempt in range(100):
            try:
                # ã€Œã‚‚ã£ã¨ã¿ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’æ¢ã™
                more_selectors = [
                    'button:has-text("ã‚‚ã£ã¨ã¿ã‚‹")',
                    'button:has-text("ã‚‚ã£ã¨è¦‹ã‚‹")',
                    '[data-testid="load-more"]'
                ]
                
                button_found = False
                for selector in more_selectors:
                    try:
                        button = await self.page.query_selector(selector)
                        if button:
                            await button.scroll_into_view_if_needed()
                            await self.page.wait_for_timeout(500)
                            await button.click()
                            print(f"  ğŸ”„ {attempt + 1}å›ç›®ã‚¯ãƒªãƒƒã‚¯")
                            button_found = True
                            await self.page.wait_for_timeout(2000)
                            break
                    except:
                        continue
                
                if not button_found:
                    # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ã¿ã‚‹
                    await self.page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                    await self.page.wait_for_timeout(2000)
                    
                    # è¨˜äº‹æ•°ãƒã‚§ãƒƒã‚¯
                    links = await self.page.query_selector_all('a[href*="/n/"]')
                    print(f"  è¨˜äº‹æ•°: {len(links)}")
                    
                    if attempt > 5:  # 5å›è©¦ã—ã¦ã ã‚ãªã‚‰çµ‚äº†
                        break
                        
            except Exception as e:
                print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
                break
                
        print("âœ… è¨˜äº‹èª­ã¿è¾¼ã¿å®Œäº†")
        
    async def collect_articles(self) -> List[Dict]:
        """è¨˜äº‹ã‚’åé›†"""
        print("ğŸ“ è¨˜äº‹ã‚’åé›†ä¸­...")
        
        articles = []
        all_links = await self.page.query_selector_all('a[href*="/n/"]')
        
        for link in all_links:
            try:
                href = await link.get_attribute('href')
                if href and '/n/' in href and re.match(r'.*/n/[a-zA-Z0-9_-]+', href):
                    full_url = urljoin(self.base_url, href)
                    full_url = full_url.split('?')[0].split('#')[0]
                    
                    if '/info/n/' not in full_url and not any(a['url'] == full_url for a in articles):
                        articles.append({'url': full_url})
            except:
                continue
                
        print(f"âœ… {len(articles)} è¨˜äº‹ã‚’åé›†")
        return articles
        
    async def scrape_article(self, url: str, index: int, total: int) -> Dict:
        """è¨˜äº‹å†…å®¹ã‚’å–å¾—"""
        print(f"ğŸ“„ {index}/{total}: {url}")
        
        try:
            await self.page.goto(url, wait_until="domcontentloaded")
            await self.page.wait_for_timeout(1000)
            
            content = await self.page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            data = {
                'url': url,
                'ã‚¿ã‚¤ãƒˆãƒ«': '',
                'æœ¬æ–‡': '',
                'å…¬é–‹æ—¥': '',
                'ä¾¡æ ¼': 'N/A',
                'è³¼å…¥çŠ¶æ³': 'è³¼å…¥æ¸ˆã¿ or ç„¡æ–™'
            }
            
            # ã‚¿ã‚¤ãƒˆãƒ«
            page_title = await self.page.title()
            if page_title and 'ï½œnote' in page_title:
                data['ã‚¿ã‚¤ãƒˆãƒ«'] = page_title.split('ï½œnote')[0].strip()
            
            # å…¬é–‹æ—¥
            time_elem = soup.select_one('time')
            if time_elem:
                data['å…¬é–‹æ—¥'] = time_elem.get('datetime', '')
                
            # æœ¬æ–‡
            paragraphs = soup.find_all('p')
            data['æœ¬æ–‡'] = '\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
            
            # æœ‰æ–™ãƒã‚§ãƒƒã‚¯
            if 'æœ‰æ–™' in content or 'ç¶šãã‚’ã¿ã‚‹ã«ã¯' in content:
                data['ä¾¡æ ¼'] = 'æœ‰æ–™'
                
            print(f"âœ… '{data['ã‚¿ã‚¤ãƒˆãƒ«'][:20]}...'")
            return data
            
        except Exception as e:
            return {
                'url': url,
                'ã‚¿ã‚¤ãƒˆãƒ«': f'ã‚¨ãƒ©ãƒ¼',
                'æœ¬æ–‡': f'ã‚¨ãƒ©ãƒ¼: {str(e)}',
                'å…¬é–‹æ—¥': '',
                'ä¾¡æ ¼': 'N/A',
                'è³¼å…¥çŠ¶æ³': 'ã‚¨ãƒ©ãƒ¼'
            }
            
    async def run(self, author_url: str):
        """å®Ÿè¡Œ"""
        await self.initialize()
        
        try:
            await self.wait_for_simple_login(author_url)
            await self.auto_load_all_articles()
            
            articles = await self.collect_articles()
            if not articles:
                print("âŒ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return
                
            print(f"\nğŸš€ {len(articles)} è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
            
            results = []
            for i, article in enumerate(articles, 1):
                result = await self.scrape_article(article['url'], i, len(articles))
                results.append(result)
                
                if i % 10 == 0:
                    print(f"ğŸ’¾ {i} è¨˜äº‹å®Œäº†")
                    
                await asyncio.sleep(1)
                
            # CSVä¿å­˜
            df = pd.DataFrame(results)
            df.insert(0, 'ç•ªå·', range(1, len(df) + 1))
            
            columns = ['ç•ªå·', 'å…¬é–‹æ—¥', 'ã‚¿ã‚¤ãƒˆãƒ«', 'æœ¬æ–‡', 'ä¾¡æ ¼', 'è³¼å…¥çŠ¶æ³']
            df = df[columns]
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M")
            filename = f"ihayato_simple_{timestamp}.csv"
            
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            file_size = os.path.getsize(filename) / 1024
            print(f"\nğŸ‰ å®Œäº†! {len(results)} è¨˜äº‹ã‚’ {filename} ã«ä¿å­˜")
            print(f"ğŸ“ ã‚µã‚¤ã‚º: {file_size:.1f} KB")
            
        finally:
            await self.close()


async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('url', help='è‘—è€…URL')
    parser.add_argument('--headless', action='store_true')
    
    args = parser.parse_args()
    
    print("ğŸš€ Note Scraper Simple é–‹å§‹")
    
    scraper = NoteScraperSimple(headless=args.headless)
    await scraper.run(args.url)


if __name__ == "__main__":
    asyncio.run(main())