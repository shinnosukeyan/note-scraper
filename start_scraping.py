#!/usr/bin/env python3
"""
ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ä¿å­˜ã•ã‚ŒãŸãƒ–ãƒ©ã‚¦ã‚¶ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ
"""

import asyncio
import argparse
import json
import os
import sys
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright

# srcãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®è¿½åŠ 
sys.path.append(str(Path(__file__).parent / 'src'))

from src.collector import ArticleCollector
from src.formatter import ContentFormatter
from src.exporter import CSVExporter
from src.scraper import NoteScraper

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
SESSION_FILE = "browser_session.json"
SETUP_DONE_FILE = "setup_done.txt"


async def start_scraping(limit: int = None):
    """ä¿å­˜ã•ã‚ŒãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã®èª­ã¿è¾¼ã¿
    if not os.path.exists(SESSION_FILE):
        print("âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print(f"ğŸ’¡ å…ˆã« prepare_browser.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return {'success': False, 'error': 'Session file not found'}
    
    # setup_done.txtã®ç¢ºèª
    if not os.path.exists(SETUP_DONE_FILE):
        print("âŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¦ã„ã¾ã›ã‚“")
        print(f"ğŸ’¡ {SETUP_DONE_FILE} ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„")
        return {'success': False, 'error': 'Setup not completed'}
    
    with open(SESSION_FILE, 'r', encoding='utf-8') as f:
        session_info = json.load(f)
    
    print("ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™")
    print(f"ğŸ“ å¯¾è±¡: {session_info['profile_url']}")
    print(f"ğŸ” ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæ™‚åˆ»: {session_info['created_at']}")
    
    # å¿…è¦ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®åˆæœŸåŒ–
    collector = ArticleCollector()
    formatter = ContentFormatter()
    exporter = CSVExporter()
    
    async with async_playwright() as p:
        # æ—¢å­˜ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ¥ç¶š
        context_dir = session_info['context_dir']
        
        try:
            # æ°¸ç¶šåŒ–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å†æ¥ç¶š
            context = await p.chromium.launch_persistent_context(
                context_dir,
                headless=False,  # æ—¢ã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ãƒ–ãƒ©ã‚¦ã‚¶ã‚’ä½¿ç”¨
                locale='ja-JP',
                viewport={'width': 1280, 'height': 800}
            )
            
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒšãƒ¼ã‚¸ã‚’å–å¾—
            if not context.pages:
                print("âŒ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return {'success': False, 'error': 'No active pages found'}
            
            page = context.pages[0]
            current_url = page.url
            
            print(f"ğŸ“„ ç¾åœ¨ã®ãƒšãƒ¼ã‚¸: {current_url}")
            
            # è¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸ã«ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
            if '/all' not in current_url:
                list_url = session_info['list_url']
                print(f"ğŸ“„ è¨˜äº‹ä¸€è¦§ã«ç§»å‹•: {list_url}")
                await page.goto(list_url, wait_until='networkidle')
            
            # è¨˜äº‹URLã®åé›†
            print("ğŸ” è¨˜äº‹URLã‚’åé›†ä¸­...")
            article_urls = await collector.collect_article_links(page)
            print(f"âœ… {len(article_urls)} è¨˜äº‹ã‚’ç™ºè¦‹")
            
            # è¨˜äº‹æ•°åˆ¶é™é©ç”¨
            if limit and len(article_urls) > limit:
                article_urls = article_urls[:limit]
                print(f"âš¡ è¨˜äº‹æ•°ã‚’ {limit} è¨˜äº‹ã«åˆ¶é™")
            
            if not article_urls:
                print("âŒ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return {'success': False, 'error': 'No articles found'}
            
            # å„è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            articles = []
            print(f"\nğŸ“„ {len(article_urls)} è¨˜äº‹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹...")
            
            for i, url in enumerate(article_urls, 1):
                print(f"ğŸ“„ è¨˜äº‹ {i}/{len(article_urls)}: {url}")
                
                try:
                    await page.goto(url, wait_until='networkidle', timeout=30000)
                    await asyncio.sleep(2)
                    
                    # ãƒšãƒ¼ã‚¸HTMLã‚’å–å¾—
                    html = await page.content()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
                    title_elem = soup.find('h1', class_='note-common-styles__textnote-title')
                    title = title_elem.get_text(strip=True) if title_elem else '...'
                    
                    # å…¬é–‹æ—¥æ™‚
                    published_elem = soup.find('time')
                    published_at = published_elem.get('datetime', '') if published_elem else ''
                    
                    # æœ¬æ–‡ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                    content = formatter.extract_formatted_content(soup)
                    
                    # ãƒãƒŠãƒ¼æ¤œå‡ºã®ãƒ‡ãƒãƒƒã‚°
                    figures = soup.find_all('figure', attrs={'embedded-service': 'external-article'})
                    if figures:
                        print(f"ğŸ” ãƒãƒŠãƒ¼æ¤œå‡º: {url}")
                    
                    # ä¾¡æ ¼æƒ…å ±
                    if soup.find('button', string=lambda x: x and 'è³¼å…¥' in x):
                        price = 'æœ‰æ–™'
                        status = 'æœªè³¼å…¥'
                    else:
                        price = 'ç„¡æ–™'
                        status = 'ç„¡æ–™'
                    
                    articles.append({
                        'url': url,
                        'title': title,
                        'published_at': published_at,
                        'content': content,
                        'price': price,
                        'status': status
                    })
                    
                    if title != '...':
                        print(f"âœ… '{title[:30]}...' ã‚’å–å¾—å®Œäº†" if len(title) > 30 else f"âœ… '{title}' ã‚’å–å¾—å®Œäº†")
                    
                except Exception as e:
                    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                    articles.append({
                        'url': url,
                        'title': 'ã‚¨ãƒ©ãƒ¼',
                        'published_at': '',
                        'content': f'å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}',
                        'price': 'N/A',
                        'status': 'ã‚¨ãƒ©ãƒ¼'
                    })
            
            # CSVä¿å­˜
            timestamp = datetime.now().strftime("%Y%m%d_%H%M")
            filename = f"output/ihayato_final_{timestamp}.csv"
            
            # outputãƒ•ã‚©ãƒ«ãƒ€ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
            os.makedirs("output", exist_ok=True)
            
            result = exporter.save_to_csv(articles, filename)
            
            print(f"\nğŸ‰ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†!")
            print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {result['filename']}")
            print(f"ğŸ“Š è¨˜äº‹æ•°: {result['article_count']}")
            print(f"ğŸ’¾ ã‚µã‚¤ã‚º: {result['file_size_mb']} MB")
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’é–‰ã˜ã‚‹
            await context.close()
            
            return result
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return {'success': False, 'error': str(e)}


def main():
    parser = argparse.ArgumentParser(description='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ')
    parser.add_argument('--limit', type=int, help='å–å¾—è¨˜äº‹æ•°ã®ä¸Šé™')
    
    args = parser.parse_args()
    
    # å®Ÿè¡Œ
    result = asyncio.run(start_scraping(args.limit))
    
    if result['success']:
        print("\nâœ… ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
    else:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ã§çµ‚äº†: {result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
        sys.exit(1)


if __name__ == "__main__":
    main()