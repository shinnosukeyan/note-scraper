#!/usr/bin/env python3
"""
HTMLå†…å®¹ã‹ã‚‰è¨˜äº‹ã‚’æŠ½å‡º
"""

import sys
import os
from datetime import datetime
from typing import List, Dict
import pandas as pd
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin

def extract_articles_from_html(html_content: str):
    """HTMLã‹ã‚‰è¨˜äº‹ã‚’æŠ½å‡º"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’åé›†
    article_links = []
    for link in soup.find_all('a', href=True):
        href = link['href']
        if '/n/' in href and not href.endswith('/n/'):
            if re.match(r'.*/n/[a-zA-Z0-9_-]+', href):
                if '/info/n/' not in href:
                    full_url = urljoin("https://note.com", href)
                    full_url = full_url.split('?')[0].split('#')[0]
                    if full_url not in article_links:
                        article_links.append(full_url)
    
    print(f"âœ… {len(article_links)} è¨˜äº‹ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
    
    # è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡º
    articles = []
    for i, url in enumerate(article_links, 1):
        title = f"è¨˜äº‹{i}"  # åŸºæœ¬ã‚¿ã‚¤ãƒˆãƒ«
        
        # URLã‹ã‚‰è¨˜äº‹IDã‚’æŠ½å‡º
        article_id = url.split('/n/')[-1] if '/n/' in url else ""
        
        articles.append({
            'ç•ªå·': i,
            'å…¬é–‹æ—¥': "",
            'ã‚¿ã‚¤ãƒˆãƒ«': title,
            'æœ¬æ–‡': f"è¨˜äº‹URL: {url}",
            'ä¾¡æ ¼': "ä¸æ˜",
            'è³¼å…¥çŠ¶æ³': "ä¸æ˜",
            'URL': url
        })
    
    return articles

def main():
    html_file = "page_source.html"
    
    if not os.path.exists(html_file):
        print(f"âŒ {html_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("ğŸ“‹ æ‰‹é †:")
        print("1. ãƒ–ãƒ©ã‚¦ã‚¶ã§å³ã‚¯ãƒªãƒƒã‚¯â†’ã€Œãƒšãƒ¼ã‚¸ã®ã‚½ãƒ¼ã‚¹ã‚’è¡¨ç¤ºã€")
        print("2. å…¨é¸æŠï¼ˆCmd+Aï¼‰â†’ã‚³ãƒ”ãƒ¼ï¼ˆCmd+Cï¼‰")
        print("3. ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ‡ã‚£ã‚¿ã§ page_source.html ã¨ã—ã¦ä¿å­˜")
        print("4. ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å†å®Ÿè¡Œ")
        return
    
    print(f"ğŸ“„ {html_file} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    with open(html_file, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    # è¨˜äº‹ã‚’æŠ½å‡º
    articles = extract_articles_from_html(html_content)
    
    if not articles:
        print("âŒ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # CSVä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    filename = f"ihayato_from_html_{timestamp}.csv"
    
    df = pd.DataFrame(articles)
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    
    print(f"ğŸ‰ å®Œäº†! {len(articles)} è¨˜äº‹ã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()