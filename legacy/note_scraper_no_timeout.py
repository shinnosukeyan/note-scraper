#!/usr/bin/env python3
"""
Note Scraper No Timeout - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç„¡ã—ç‰ˆ
æ‰‹å‹•ä½œæ¥­ã‚’çµ¶å¯¾ã«é‚ªé­”ã—ãªã„è¨­è¨ˆ
"""

import asyncio
import sys
import os
from datetime import datetime
from typing import List, Dict, Optional
import pandas as pd
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Page, Browser
import argparse
import json
import re
import time
from urllib.parse import urljoin, urlparse


class NoteScraperNoTimeout:
    """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç„¡ã—ç‰ˆNoteã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.base_url = "https://note.com"
        
    async def initialize(self):
        """ãƒ–ãƒ©ã‚¦ã‚¶ã‚’åˆæœŸåŒ–"""
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(
            headless=self.headless,
            args=[
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-web-security',
                '--disable-dev-shm-usage',
                '--no-first-run'
            ]
        )
        context = await self.browser.new_context(
            viewport={'width': 1280, 'height': 720},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        )
        
        # é‡è¦ï¼šå…¨ã¦ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’ç„¡åŠ¹åŒ–
        context.set_default_timeout(0)  # 0 = ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç„¡ã—
        
        self.page = await context.new_page()
        
        # ãƒšãƒ¼ã‚¸ãƒ¬ãƒ™ãƒ«ã§ã‚‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’ç„¡åŠ¹åŒ–
        self.page.set_default_timeout(0)
        
    async def navigate_to_profile(self, profile_url: str):
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã«ç§»å‹•"""
        print(f"ğŸŒ ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã«ç§»å‹•: {profile_url}")
        
        try:
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç„¡ã—ã§ç§»å‹•
            await self.page.goto(profile_url, wait_until="domcontentloaded", timeout=0)
            await self.page.wait_for_timeout(3000)
            
            # è¨˜äº‹ä¸€è¦§ã‚¿ãƒ–ã«ç§»å‹•
            article_list_url = profile_url.rstrip('/') + '/all'
            print(f"ğŸ“„ è¨˜äº‹ä¸€è¦§ã«ç§»å‹•: {article_list_url}")
            await self.page.goto(article_list_url, wait_until="domcontentloaded", timeout=0)
            await self.page.wait_for_timeout(3000)
            
        except Exception as e:
            print(f"âŒ ãƒšãƒ¼ã‚¸ç§»å‹•ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def wait_for_manual_setup(self):
        """æ‰‹å‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ã¾ã§å¾…æ©Ÿ"""
        print("\n" + "="*60)
        print("ğŸ› ï¸  æ‰‹å‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå¿…è¦ã§ã™")
        print("="*60)
        print()
        print("ğŸ“‹ æ‰‹é †:")
        print("1. ãƒ–ãƒ©ã‚¦ã‚¶ã§ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„")
        print("2. ã€Œã‚‚ã£ã¨ã¿ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’å…¨éƒ¨ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„")
        print("3. å…¨è¨˜äº‹ãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€åˆ¥ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ä»¥ä¸‹ã‚’å®Ÿè¡Œ:")
        print("   touch /Users/yusukeohata/Desktop/development/note-scraper/setup_done.txt")
        print()
        print("âš ï¸  é‡è¦: ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯æ‰‹å‹•ä½œæ¥­ã‚’é‚ªé­”ã—ã¾ã›ã‚“")
        print("   ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¯è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("   ãƒ–ãƒ©ã‚¦ã‚¶ã¯è‡ªå‹•ã§é–‰ã˜ã¾ã›ã‚“")
        print()
        
        setup_file = "/Users/yusukeohata/Desktop/development/note-scraper/setup_done.txt"
        
        # setup_done.txtãƒ•ã‚¡ã‚¤ãƒ«ãŒã§ãã‚‹ã¾ã§å¾…æ©Ÿ
        while True:
            if os.path.exists(setup_file):
                print("âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ã‚’ç¢ºèªã—ã¾ã—ãŸ")
                os.remove(setup_file)  # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                break
            
            print("â° æ‰‹å‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’å¾…æ©Ÿä¸­... (Ctrl+C ã§ä¸­æ–­)")
            await asyncio.sleep(5)  # 5ç§’ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
    
    async def collect_all_articles(self) -> List[Dict]:
        """ç¾åœ¨è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹å…¨è¨˜äº‹ã‚’åé›†"""
        print("\nğŸ“ è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹å…¨è¨˜äº‹ã‚’åé›†ä¸­...")
        
        articles = []
        
        # å°‘ã—å¾…æ©Ÿã—ã¦ã‹ã‚‰åé›†é–‹å§‹
        await self.page.wait_for_timeout(2000)
        
        # å…¨è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’å–å¾—
        all_links = await self.page.query_selector_all('a[href*="/n/"]')
        
        for link in all_links:
            try:
                if await self.is_article_link(link):
                    href = await link.get_attribute('href')
                    full_url = urljoin(self.base_url, href)
                    full_url = full_url.split('?')[0].split('#')[0]
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if not any(article['url'] == full_url for article in articles):
                        articles.append({
                            'url': full_url,
                            'title': '',
                            'content': '',
                            'date': '',
                            'price': '',
                            'purchase_status': ''
                        })
                        
            except Exception as e:
                continue
        
        print(f"ğŸ¯ åé›†ã—ãŸè¨˜äº‹æ•°: {len(articles)}")
        return articles
    
    async def is_article_link(self, link):
        """æœ‰åŠ¹ãªè¨˜äº‹ãƒªãƒ³ã‚¯ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            href = await link.get_attribute('href')
            if href and '/n/' in href and not href.endswith('/n/'):
                if re.match(r'.*/n/[a-zA-Z0-9_-]+', href):
                    return '/info/n/' not in href
        except:
            pass
        return False
    
    async def scrape_article(self, article: Dict) -> Dict:
        """è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        url = article['url']
        
        try:
            print(f"ğŸ“„ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­: {url}")
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç„¡ã—ã§ç§»å‹•
            await self.page.goto(url, wait_until="domcontentloaded", timeout=0)
            await self.page.wait_for_timeout(2000)
            
            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ï¼‰
            page_title = await self.page.title()
            if page_title and 'ï½œnote' in page_title:
                article['title'] = page_title.split('ï½œnote')[0].strip()
            
            # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’å–å¾—
            content = await self.page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            # æœ¬æ–‡å–å¾—
            article_body = soup.find('div', class_='note-common-styles__textnote-body')
            if article_body:
                article['content'] = article_body.get_text(strip=True)
            
            # å…¬é–‹æ—¥å–å¾—
            date_element = soup.find('time')
            if date_element and date_element.get('datetime'):
                article['date'] = date_element['datetime']
            
            # ä¾¡æ ¼æƒ…å ±å–å¾—
            price_element = soup.find('span', string=re.compile(r'ï¿¥|å††'))
            if price_element:
                article['price'] = 'æœ‰æ–™'
                article['purchase_status'] = 'è³¼å…¥æ¸ˆã¿ or ç„¡æ–™'
            else:
                article['price'] = 'ç„¡æ–™'
                article['purchase_status'] = 'ç„¡æ–™'
            
            print(f"âœ… å®Œäº†: {article['title'][:50]}...")
            
        except Exception as e:
            print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {url} - {e}")
            article['title'] = f"ã‚¨ãƒ©ãƒ¼: {e}"
        
        return article
    
    async def save_to_csv(self, articles: List[Dict], filename: str):
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        print(f"ğŸ’¾ CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜: {filename}")
        
        # DataFrameã«å¤‰æ›
        df_data = []
        for i, article in enumerate(articles, 1):
            df_data.append({
                'ç•ªå·': i,
                'å…¬é–‹æ—¥': article['date'],
                'ã‚¿ã‚¤ãƒˆãƒ«': article['title'],
                'æœ¬æ–‡': article['content'],
                'ä¾¡æ ¼': article['price'],
                'è³¼å…¥çŠ¶æ³': article['purchase_status']
            })
        
        df = pd.DataFrame(df_data)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… ä¿å­˜å®Œäº†: {len(articles)}è¨˜äº‹")
    
    async def run(self, profile_url: str):
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        try:
            await self.initialize()
            await self.navigate_to_profile(profile_url)
            
            # æ‰‹å‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¾…æ©Ÿ
            await self.wait_for_manual_setup()
            
            # è¨˜äº‹åé›†
            articles = await self.collect_all_articles()
            
            if not articles:
                print("âŒ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return
            
            # å„è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            for article in articles:
                await self.scrape_article(article)
                await asyncio.sleep(1.5)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            timestamp = datetime.now().strftime("%Y%m%d_%H%M")
            filename = f"ihayato_no_timeout_{timestamp}.csv"
            await self.save_to_csv(articles, filename)
            
            print(f"\nğŸ‰ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†!")
            print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {filename}")
            print(f"ğŸ“Š è¨˜äº‹æ•°: {len(articles)}")
            
        except KeyboardInterrupt:
            print("\nâ¸ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            if self.browser:
                await self.browser.close()


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='Note Scraper No Timeout')
    parser.add_argument('url', help='ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«URL')
    parser.add_argument('--headless', action='store_true', help='ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰')
    
    args = parser.parse_args()
    
    scraper = NoteScraperNoTimeout(headless=args.headless)
    await scraper.run(args.url)


if __name__ == "__main__":
    asyncio.run(main())