#!/usr/bin/env python3
"""
Note Scraper Stable - å®‰å®šç‰ˆ
ãƒ­ã‚°ã‚¤ãƒ³å•é¡Œã‚’ä¿®æ­£ã—ã€ä¿å­˜å…ˆã‚’æ˜ç¢ºã«ã—ãŸç‰ˆæœ¬
"""

import asyncio
import sys
import os
from datetime import datetime
from typing import List, Dict, Optional
import pandas as pd
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Page, Browser
import argparse
import json
import re
import time
from urllib.parse import urljoin, urlparse


class NoteScraperStable:
    """å®‰å®šç‰ˆNoteã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self, headless: bool = True):
        self.headless = headless
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.base_url = "https://note.com"
        
    async def initialize(self):
        """ãƒ–ãƒ©ã‚¦ã‚¶ã‚’åˆæœŸåŒ–ï¼ˆå®‰å®šåŒ–è¨­å®šï¼‰"""
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(
            headless=self.headless,
            args=[
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-first-run',
                '--disable-background-timer-throttling',
                '--disable-backgrounding-occluded-windows',
                '--disable-renderer-backgrounding',
                '--disable-field-trial-config'
            ],
            slow_mo=100  # 100ms ã®é…å»¶ã‚’è¿½åŠ ã—ã¦å®‰å®šæ€§å‘ä¸Š
        )
        
        # ã‚ˆã‚Šå®‰å®šã—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
        context = await self.browser.new_context(
            viewport={'width': 1280, 'height': 720},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            extra_http_headers={
                'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8'
            }
        )
        
        self.page = await context.new_page()
        
        # ãƒšãƒ¼ã‚¸ã‚¤ãƒ™ãƒ³ãƒˆã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        self.page.on("dialog", lambda dialog: asyncio.create_task(dialog.dismiss()))
        
    async def close(self):
        """ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã‚‹"""
        if self.browser:
            await self.browser.close()
            
    async def wait_for_login_stable(self):
        """æ”¹å–„ç‰ˆæ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³å¾…æ©Ÿ"""
        print("\n" + "="*80)
        print("ğŸ” æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰ - æœ‰æ–™è¨˜äº‹å–å¾—ã®ãŸã‚")
        print("="*80)
        print("ğŸ“‹ æ‰‹é †:")
        print("1. ãƒ–ãƒ©ã‚¦ã‚¶ã§note.comãŒé–‹ãã¾ã™")
        print("2. å³ä¸Šã®ã€Œãƒ­ã‚°ã‚¤ãƒ³ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯")
        print("3. ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒ»ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’æ‰‹å‹•å…¥åŠ›")
        print("4. âš ï¸  é‡è¦: ã‚¿ãƒ–ã‚­ãƒ¼ã¯ä½¿ã‚ãšã€å…¨ã¦ãƒã‚¦ã‚¹ã§ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„")
        print("5. âš ï¸  äºˆæ¸¬å¤‰æ›ãŒå‡ºãŸå ´åˆ:")
        print("   - äºˆæ¸¬å¤‰æ›ã‚’ã‚¯ãƒªãƒƒã‚¯ã›ãšã€æœ€å¾Œã¾ã§æ‰‹å…¥åŠ›ã—ã¦ãã ã•ã„")
        print("   - ã¾ãŸã¯ã€Escã‚­ãƒ¼ã§äºˆæ¸¬å¤‰æ›ã‚’é–‰ã˜ã¦ã‹ã‚‰ç¶šè¡Œã—ã¦ãã ã•ã„")
        print("6. ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†å¾Œã€ã‚¤ã‚±ãƒãƒ¤ã•ã‚“ã®æœ‰æ–™è¨˜äº‹ãŒèª­ã‚ã‚‹çŠ¶æ…‹ã«ã—ã¦ãã ã•ã„")
        print("7. ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†ã—ãŸã‚‰ã€ä¸‹è¨˜ã®æ–¹æ³•ã§é€šçŸ¥ã—ã¦ãã ã•ã„")
        print("="*80)
        print("ğŸ’¡ ãƒ­ã‚°ã‚¤ãƒ³å¾Œã€æœ‰æ–™è¨˜äº‹ã®æœ¬æ–‡ãŒè¦‹ãˆã‚‹ã‹ãƒ†ã‚¹ãƒˆãƒšãƒ¼ã‚¸ã§ç¢ºèªã—ã¦ãã ã•ã„")
        print("="*80)
        
        # Noteã®ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã«ç§»å‹•
        try:
            await self.page.goto(self.base_url, wait_until="domcontentloaded", timeout=30000)
            await self.page.wait_for_timeout(3000)
            print("âœ… note.comã‚’é–‹ãã¾ã—ãŸ - ãƒ–ãƒ©ã‚¦ã‚¶ã§ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„")
        except Exception as e:
            print(f"âŒ ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’å¾…æ©Ÿï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        print("\nğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†å¾Œã€ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®æ–¹æ³•ã§ç¶šè¡Œã—ã¦ãã ã•ã„:")
        print("   1. ã“ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§Enterã‚­ãƒ¼ã‚’æŠ¼ã™")
        print("   2. ã¾ãŸã¯ã€login_done.txt ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹")
        print(f"   ä¾‹: touch {os.getcwd()}/login_done.txt")
        
        try:
            input()
        except EOFError:
            print("â³ ãƒ•ã‚¡ã‚¤ãƒ«å¾…æ©Ÿãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™...")
            login_file = os.path.join(os.getcwd(), "login_done.txt")
            print(f"ğŸ“ {login_file} ã®ä½œæˆã‚’å¾…æ©Ÿä¸­...")
            
            import time
            max_wait_time = 600  # 10åˆ†é–“å¾…æ©Ÿ
            elapsed_time = 0
            
            while not os.path.exists(login_file) and elapsed_time < max_wait_time:
                time.sleep(1)
                elapsed_time += 1
                if elapsed_time % 30 == 0:  # 30ç§’ã”ã¨ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
                    print(f"\nâ° å¾…æ©Ÿä¸­... ({elapsed_time//60}åˆ†{elapsed_time%60}ç§’çµŒé)")
                    print("ğŸ’¡ ãƒ–ãƒ©ã‚¦ã‚¶ãŒé–‰ã˜ãŸå ´åˆã¯ã€Ctrl+Cã§çµ‚äº†ã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„")
                else:
                    print(".", end="", flush=True)
            
            if elapsed_time >= max_wait_time:
                print("\nâ° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: 10åˆ†çµŒéã—ã¾ã—ãŸ")
                return
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            os.remove(login_file)
            print("\nâœ… ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†ã‚’ç¢ºèªã—ã¾ã—ãŸ")
        
        # ãƒ­ã‚°ã‚¤ãƒ³ç¢ºèª
        try:
            await self.page.wait_for_timeout(2000)
            
            # ã‚ˆã‚Šè©³ç´°ãªãƒ­ã‚°ã‚¤ãƒ³ç¢ºèª
            current_url = self.page.url
            content = await self.page.content()
            
            print(f"ğŸŒ ç¾åœ¨ã®URL: {current_url}")
            
            # ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã®è©³ç´°ç¢ºèª
            login_indicators = [
                ('ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ', 'ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ'),
                ('ãƒã‚¤ãƒšãƒ¼ã‚¸', 'mypage'),
                ('è¨­å®š', 'settings'),
                ('ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ', 'account'),
                ('ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«', 'profile')
            ]
            
            logged_in = False
            for indicator, desc in login_indicators:
                if indicator in content.lower():
                    print(f"âœ… ãƒ­ã‚°ã‚¤ãƒ³ç¢ºèª: {desc}ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                    logged_in = True
                    break
            
            if not logged_in:
                print("âš ï¸  æ˜ç¢ºãªãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ç¢ºèªã§ãã¾ã›ã‚“ãŒã€å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™")
                print("ğŸ’¡ æœ‰æ–™è¨˜äº‹ãŒæ­£ã—ãå–å¾—ã§ããªã„å ´åˆã¯ã€ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’å†ç¢ºèªã—ã¦ãã ã•ã„")
            else:
                print("ğŸ‰ ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ç¢ºèªã—ã¾ã—ãŸï¼æœ‰æ–™è¨˜äº‹ã®å–å¾—ãŒå¯èƒ½ã§ã™")
                
        except Exception as e:
            print(f"âš ï¸  ãƒ­ã‚°ã‚¤ãƒ³ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ’¡ å‡¦ç†ã¯ç¶šè¡Œã—ã¾ã™ãŒã€æœ‰æ–™è¨˜äº‹ãŒå–å¾—ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            
    async def get_author_articles_stable(self, author_url: str) -> List[Dict]:
        """è‘—è€…ã®è¨˜äº‹ä¸€è¦§ã‚’å®‰å®šå–å¾—"""
        articles = []
        page_num = 1
        max_pages = 50  # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
        
        print(f"\nğŸ“– è‘—è€…ãƒšãƒ¼ã‚¸ã‚’èª¿æŸ»ä¸­: {author_url}")
        
        # è¨˜äº‹ã‚¿ãƒ–ã«ç§»å‹•ï¼ˆ/all ã‚’è¿½åŠ ï¼‰
        if not author_url.endswith('/all'):
            author_url = author_url.rstrip('/') + '/all'
            print(f"ğŸ“ è¨˜äº‹ã‚¿ãƒ–ã«ç§»å‹•: {author_url}")
        
        while page_num <= max_pages:
            try:
                # ãƒšãƒ¼ã‚¸URLã®æ§‹ç¯‰
                if '?' in author_url:
                    url = f"{author_url}&page={page_num}"
                else:
                    url = f"{author_url}?page={page_num}"
                    
                print(f"ãƒšãƒ¼ã‚¸ {page_num} ã‚’å–å¾—ä¸­...")
                await self.page.goto(url, wait_until="domcontentloaded", timeout=30000)
                await self.page.wait_for_timeout(3000)
                
                # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦è¿½åŠ è¨˜äº‹ã‚’èª­ã¿è¾¼ã‚€ï¼ˆç„¡é™ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾å¿œï¼‰
                print("  - è¿½åŠ è¨˜äº‹ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                previous_article_count = 0
                scroll_attempts = 0
                max_scroll_attempts = 5
                
                while scroll_attempts < max_scroll_attempts:
                    # ç¾åœ¨ã®è¨˜äº‹æ•°ã‚’å–å¾—
                    current_links = await self.page.query_selector_all('a[href*="/n/"]')
                    current_count = 0
                    for link in current_links:
                        href = await link.get_attribute('href')
                        if href and '/n/' in href and not href.endswith('/n/'):
                            if re.match(r'.*/n/[a-zA-Z0-9_-]+', href):
                                full_url = urljoin(self.base_url, href)
                                if '/info/n/' not in full_url:
                                    current_count += 1
                    
                    if current_count > previous_article_count:
                        print(f"    è¨˜äº‹æ•°: {previous_article_count} â†’ {current_count}")
                        previous_article_count = current_count
                        scroll_attempts = 0  # è¨˜äº‹ãŒå¢—ãˆãŸã‚‰ãƒªã‚»ãƒƒãƒˆ
                    else:
                        scroll_attempts += 1
                    
                    # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦è¿½åŠ èª­ã¿è¾¼ã¿
                    await self.page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                    await self.page.wait_for_timeout(1000)
                    
                    # èª­ã¿è¾¼ã¿ä¸­ã®è¡¨ç¤ºãŒã‚ã‚Œã°å¾…æ©Ÿ
                    loading_element = await self.page.query_selector('.loading, .spinner, [aria-label="èª­ã¿è¾¼ã¿ä¸­"]')
                    if loading_element:
                        await self.page.wait_for_timeout(3000)
                        
                print(f"  - æœ€çµ‚è¨˜äº‹æ•°: {previous_article_count}")
                
                # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¾Œã«å…¨ã¦ã®è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’å–å¾—
                article_links = []
                all_links = await self.page.query_selector_all('a[href]')
                debug_count = 0
                excluded_count = 0
                
                for link in all_links:
                    href = await link.get_attribute('href')
                    if href and '/n/' in href and not href.endswith('/n/'):
                        debug_count += 1
                        # noteè¨˜äº‹ã®URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
                        if re.match(r'.*/n/[a-zA-Z0-9_-]+', href):
                            full_url = urljoin(self.base_url, href)
                            # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»
                            full_url = full_url.split('?')[0].split('#')[0]
                            # æ±ç”¨çš„ãªè¨˜äº‹ãƒªãƒ³ã‚¯ã‚’é™¤å¤–
                            if '/info/n/' not in full_url:
                                if not any(a['url'] == full_url for a in article_links):
                                    article_links.append({'url': full_url})
                                    # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®10è¨˜äº‹ã®URLã‚’å‡ºåŠ›
                                    if len(article_links) <= 10:
                                        print(f"    è¿½åŠ : {full_url}")
                                else:
                                    excluded_count += 1
                            else:
                                excluded_count += 1
                        else:
                            excluded_count += 1
                
                print(f"  - ãƒ‡ãƒãƒƒã‚°: /n/ãƒªãƒ³ã‚¯ç·æ•°: {debug_count}, é™¤å¤–: {excluded_count}, æœ‰åŠ¹: {len(article_links)}")
                
                print(f"  - å–å¾—ã—ãŸãƒªãƒ³ã‚¯æ•°: {len(article_links)}")
                                
                if not article_links:
                    print(f"ãƒšãƒ¼ã‚¸ {page_num}: è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - çµ‚äº†")
                    break
                    
                print(f"ãƒšãƒ¼ã‚¸ {page_num}: {len(article_links)} è¨˜äº‹ã‚’ç™ºè¦‹")
                
                # é‡è¤‡ã‚’é¿ã‘ã¦è¿½åŠ 
                for article in article_links:
                    if not any(a['url'] == article['url'] for a in articles):
                        articles.append(article)
                
                # æ¬¡ã®ãƒšãƒ¼ã‚¸ã®ç¢ºèªï¼ˆã‚ˆã‚Šåºƒç¯„å›²ãªã‚»ãƒ¬ã‚¯ã‚¿ï¼‰
                next_selectors = [
                    'a[rel="next"]',
                    '.pagination a[aria-label="Next"]',
                    '.pagination a[aria-label="æ¬¡ã¸"]',
                    'a[href*="page="]',
                    '.pagination .next'
                ]
                
                next_exists = None
                for selector in next_selectors:
                    try:
                        next_exists = await self.page.query_selector(selector)
                        if next_exists:
                            break
                    except:
                        continue
                
                if not next_exists:
                    print("æœ€å¾Œã®ãƒšãƒ¼ã‚¸ã«åˆ°é”ã—ã¾ã—ãŸ")
                    break
                    
                page_num += 1
                await asyncio.sleep(2)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
                
            except Exception as e:
                print(f"ãƒšãƒ¼ã‚¸ {page_num} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                break
                
        print(f"\nâœ… åˆè¨ˆ {len(articles)} è¨˜äº‹ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
        return articles
        
    async def scrape_article_content(self, url: str, index: int, total: int) -> Dict:
        """è¨˜äº‹å†…å®¹ã‚’å–å¾—"""
        print(f"ğŸ“„ è¨˜äº‹ {index}/{total}: {url}")
        
        try:
            await self.page.goto(url, wait_until="domcontentloaded", timeout=30000)
            await self.page.wait_for_timeout(2000)
            
            # HTMLã‚’å–å¾—ã—ã¦è§£æ
            content = await self.page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            data = {
                'url': url,
                'ã‚¿ã‚¤ãƒˆãƒ«': '',
                'æœ¬æ–‡': '',
                'å…¬é–‹æ—¥': '',
                'ä¾¡æ ¼': 'N/A',
                'è³¼å…¥çŠ¶æ³': 'è³¼å…¥æ¸ˆã¿ or ç„¡æ–™'
            }
            
            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—ï¼ˆã‚ˆã‚Šå¹…åºƒã„ã‚»ãƒ¬ã‚¯ã‚¿ã§è©¦è¡Œï¼‰
            title_selectors = [
                'h1', 
                '.p-article__title', 
                '.note-common-styles__textnote-title',
                '[data-testid="article-title"]',
                '.o-articleTitle',
                'article h1',
                '.note-title',
                'title'
            ]
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title_text = title_elem.get_text(strip=True)
                    if title_text and title_text != 'note':  # noteã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¿ã‚¤ãƒˆãƒ«ã‚’é™¤å¤–
                        data['ã‚¿ã‚¤ãƒˆãƒ«'] = title_text
                        break
                        
            # ã‚¿ã‚¤ãƒˆãƒ«ãŒå–å¾—ã§ããªã„å ´åˆã€ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æŠ½å‡º
            if not data['ã‚¿ã‚¤ãƒˆãƒ«']:
                page_title = await self.page.title()
                if page_title and 'ï½œnote' in page_title:
                    data['ã‚¿ã‚¤ãƒˆãƒ«'] = page_title.split('ï½œnote')[0].strip()
                elif page_title:
                    data['ã‚¿ã‚¤ãƒˆãƒ«'] = page_title
                    
            # å…¬é–‹æ—¥å–å¾—
            time_elem = soup.select_one('time')
            if time_elem:
                data['å…¬é–‹æ—¥'] = time_elem.get('datetime', time_elem.get_text(strip=True))
                
            # æœ¬æ–‡å–å¾—
            content_text = await self._extract_content_stable(soup)
            data['æœ¬æ–‡'] = content_text
            
            # æœ‰æ–™è¨˜äº‹ãƒã‚§ãƒƒã‚¯
            if any(phrase in content for phrase in ['æœ‰æ–™', 'ã“ã®è¨˜äº‹ã¯æœ‰æ–™', 'ç¶šãã‚’ã¿ã‚‹ã«ã¯']):
                data['ä¾¡æ ¼'] = 'æœ‰æ–™'
                if 'ã“ã®ç¶šãã‚’ã¿ã‚‹ã«ã¯' in content:
                    data['è³¼å…¥çŠ¶æ³'] = 'æœªè³¼å…¥'
                    
            print(f"âœ… '{data['ã‚¿ã‚¤ãƒˆãƒ«'][:30]}...' ã‚’å–å¾—å®Œäº†")
            return data
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {
                'url': url,
                'ã‚¿ã‚¤ãƒˆãƒ«': f'ã‚¨ãƒ©ãƒ¼: {url}',
                'æœ¬æ–‡': f'å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}',
                'å…¬é–‹æ—¥': '',
                'ä¾¡æ ¼': 'N/A',
                'è³¼å…¥çŠ¶æ³': 'ã‚¨ãƒ©ãƒ¼'
            }
            
    async def _extract_content_stable(self, soup: BeautifulSoup) -> str:
        """æœ¬æ–‡ã‚’å®‰å®šå–å¾—"""
        # æœ¬æ–‡ã‚³ãƒ³ãƒ†ãƒŠã‚’æ¢ã™
        content_selectors = [
            '.note-common-styles__textnote-body',
            '.p-article__content', 
            'article .content',
            '.note-body'
        ]
        
        article_body = None
        for selector in content_selectors:
            article_body = soup.select_one(selector)
            if article_body:
                break
                
        if not article_body:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚ˆã‚Šåºƒç¯„å›²ã§æ¢ã™
            article_body = soup.find('main') or soup.find('article') or soup
            
        # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        paragraphs = []
        
        # æ®µè½ã”ã¨ã«å‡¦ç†
        for element in article_body.find_all(['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            text = element.get_text(strip=True)
            if text and len(text) > 10:  # çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯é™¤å¤–
                # å¤ªå­—ã®å‡¦ç†
                if element.find(['strong', 'b']):
                    for bold in element.find_all(['strong', 'b']):
                        bold_text = bold.get_text(strip=True)
                        if bold_text:
                            text = text.replace(bold_text, f'**{bold_text}**')
                            
                paragraphs.append(text)
                
        # ãƒªãƒ³ã‚¯ã®å‡¦ç†
        for link in article_body.find_all('a', href=True):
            link_text = link.get_text(strip=True)
            href = link.get('href')
            if link_text and href:
                paragraphs.append(f'[{link_text}]({href})')
                
        # ç”»åƒã®å‡¦ç†
        for img in article_body.find_all('img'):
            alt = img.get('alt', 'ç”»åƒ')
            src = img.get('src', '')
            if src:
                paragraphs.append(f'[ç”»åƒ: {alt}]\n{src}')
                
        return '\n\n'.join(paragraphs)
        
    async def scrape_author_complete(self, author_url: str, output_filename: str) -> str:
        """è‘—è€…ã®å…¨è¨˜äº‹ã‚’å®Œå…¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        full_path = os.path.abspath(output_filename)
        print(f"\nğŸ¯ å‡ºåŠ›å…ˆ: {full_path}")
        
        # è¨˜äº‹ä¸€è¦§ã‚’å–å¾—
        articles = await self.get_author_articles_stable(author_url)
        
        if not articles:
            print("âŒ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return full_path
            
        # å„è¨˜äº‹ã‚’å–å¾—
        results = []
        for i, article in enumerate(articles, 1):
            result = await self.scrape_article_content(article['url'], i, len(articles))
            results.append(result)
            await asyncio.sleep(1.5)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
            
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
        df = pd.DataFrame(results)
        
        # å…¬é–‹æ—¥ã§ã‚½ãƒ¼ãƒˆ
        if 'å…¬é–‹æ—¥' in df.columns:
            df['å…¬é–‹æ—¥_sort'] = pd.to_datetime(df['å…¬é–‹æ—¥'], errors='coerce')
            df = df.sort_values('å…¬é–‹æ—¥_sort', na_position='last')
            df = df.drop('å…¬é–‹æ—¥_sort', axis=1)
            
        # é€šã—ç•ªå·ã‚’è¿½åŠ 
        df.insert(0, 'ç•ªå·', range(1, len(df) + 1))
        
        # åˆ—ã®é †åºã‚’æ­£ã—ãè¨­å®š: ç•ªå·,å…¬é–‹æ—¥,ã‚¿ã‚¤ãƒˆãƒ«,æœ¬æ–‡,ä¾¡æ ¼,è³¼å…¥çŠ¶æ³
        columns_order = ['ç•ªå·', 'å…¬é–‹æ—¥', 'ã‚¿ã‚¤ãƒˆãƒ«', 'æœ¬æ–‡', 'ä¾¡æ ¼', 'è³¼å…¥çŠ¶æ³']
        existing_columns = [col for col in columns_order if col in df.columns]
        df = df[existing_columns]
        
        # ä¸è¦ãªåˆ—ã‚’å‰Šé™¤
        if 'url' in df.columns:
            df = df.drop('url', axis=1)
            
        # CSVä¿å­˜
        df.to_csv(output_filename, index=False, encoding='utf-8-sig')
        
        print(f"\nğŸ‰ å®Œäº†! {len(df)} è¨˜äº‹ã‚’ {full_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(output_filename) / 1024:.1f} KB")
        
        return full_path


async def main():
    parser = argparse.ArgumentParser(description="Noteè‘—è€…è¨˜äº‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆå®‰å®šç‰ˆï¼‰")
    parser.add_argument("author_url", help="è‘—è€…ã®Noteãƒšãƒ¼ã‚¸URL")
    parser.add_argument("-o", "--output", help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å")
    parser.add_argument("--no-headless", action="store_true", help="ãƒ–ãƒ©ã‚¦ã‚¶ã‚’è¡¨ç¤º")
    parser.add_argument("--login", action="store_true", help="æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³")
    
    args = parser.parse_args()
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã®æ±ºå®š
    if args.output:
        if not args.output.endswith('.csv'):
            output_filename = f"{args.output}.csv"
        else:
            output_filename = args.output
    else:
        author_name = urlparse(args.author_url).path.strip('/')
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        output_filename = f"{author_name}_{timestamp}.csv"
        
    print(f"ğŸš€ Note Scraper å®‰å®šç‰ˆã‚’é–‹å§‹")
    print(f"ğŸ“ å¯¾è±¡: {args.author_url}")
    print(f"ğŸ’¾ å‡ºåŠ›: {os.path.abspath(output_filename)}")
    
    scraper = NoteScraperStable(headless=not args.no_headless)
    
    try:
        await scraper.initialize()
        
        if args.login:
            await scraper.wait_for_login_stable()
            
        output_path = await scraper.scrape_author_complete(args.author_url, output_filename)
        
        print(f"\nâœ… å‡¦ç†å®Œäº†!")
        print(f"ğŸ“‚ ä¿å­˜å…ˆ: {output_path}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
    finally:
        await scraper.close()


if __name__ == "__main__":
    asyncio.run(main())