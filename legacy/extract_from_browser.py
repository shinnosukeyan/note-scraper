#!/usr/bin/env python3
"""
æ—¢å­˜ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ã®ç›´æ¥æŠ½å‡º
"""

import asyncio
import sys
import os
from datetime import datetime
from typing import List, Dict, Optional
import pandas as pd
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
import re
from urllib.parse import urljoin

async def main():
    print("ğŸ“‹ æ—¢å­˜ãƒ–ãƒ©ã‚¦ã‚¶ã«æ¥ç¶šã—ã¦è¨˜äº‹ã‚’æŠ½å‡ºã—ã¾ã™")
    print("âš ï¸  æ³¨æ„: æ—¢ã«é–‹ã„ã¦ã„ã‚‹ãƒ–ãƒ©ã‚¦ã‚¶ã‚’ä½¿ç”¨ã—ã¾ã™")
    
    # æ—¢å­˜ã®ãƒ–ãƒ©ã‚¦ã‚¶ã«æ¥ç¶š
    async with async_playwright() as p:
        # æ—¢å­˜ã®Chromeãƒ–ãƒ©ã‚¦ã‚¶ã«æ¥ç¶šã‚’è©¦è¡Œ
        try:
            browser = await p.chromium.connect_over_cdp("http://localhost:9222")
            print("âœ… æ—¢å­˜ãƒ–ãƒ©ã‚¦ã‚¶ã«æ¥ç¶šã—ã¾ã—ãŸ")
        except:
            print("âŒ æ—¢å­˜ãƒ–ãƒ©ã‚¦ã‚¶ã«æ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸ")
            print("ğŸ’¡ æ–°ã—ã„ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ã—ã¾ã™")
            browser = await p.chromium.launch(headless=False)
            context = await browser.new_context()
            page = await context.new_page()
            await page.goto("https://note.com/ihayato/all")
            print("âœ… æ–°ã—ã„ãƒ–ãƒ©ã‚¦ã‚¶ã§ãƒšãƒ¼ã‚¸ã‚’é–‹ãã¾ã—ãŸ")
            print("ğŸ”„ æ‰‹å‹•ã§ãƒ­ã‚°ã‚¤ãƒ³â†’ã‚‚ã£ã¨ã¿ã‚‹å…¨ã‚¯ãƒªãƒƒã‚¯å¾Œã€Enterã‚’æŠ¼ã—ã¦ãã ã•ã„")
            input()
        
        # æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—
        contexts = browser.contexts
        if contexts:
            pages = await contexts[0].pages()
            if pages:
                page = pages[0]
                print(f"ğŸ“„ ç¾åœ¨ã®ãƒšãƒ¼ã‚¸: {await page.url()}")
                
                # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’è§£æ
                content = await page.content()
                soup = BeautifulSoup(content, 'html.parser')
                
                # è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’åé›†
                article_links = []
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if '/n/' in href and not href.endswith('/n/'):
                        if re.match(r'.*/n/[a-zA-Z0-9_-]+', href):
                            if '/info/n/' not in href:
                                full_url = urljoin("https://note.com", href)
                                full_url = full_url.split('?')[0].split('#')[0]
                                if full_url not in article_links:
                                    article_links.append(full_url)
                
                print(f"âœ… {len(article_links)} è¨˜äº‹ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
                
                # å„è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                articles = []
                for i, url in enumerate(article_links, 1):
                    print(f"ğŸ“„ è¨˜äº‹ {i}/{len(article_links)}: {url}")
                    
                    try:
                        await page.goto(url)
                        await page.wait_for_timeout(2000)
                        
                        # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                        title = await page.title()
                        if title and 'ï½œnote' in title:
                            title = title.split('ï½œnote')[0].strip()
                        
                        # æœ¬æ–‡å–å¾—
                        content = await page.content()
                        soup = BeautifulSoup(content, 'html.parser')
                        
                        article_body = soup.find('div', class_='note-common-styles__textnote-body')
                        body_text = article_body.get_text(strip=True) if article_body else ""
                        
                        # æ—¥ä»˜å–å¾—
                        date_element = soup.find('time')
                        date = date_element['datetime'] if date_element and date_element.get('datetime') else ""
                        
                        # ä¾¡æ ¼æƒ…å ±
                        price_element = soup.find('span', string=re.compile(r'ï¿¥|å††'))
                        price = "æœ‰æ–™" if price_element else "ç„¡æ–™"
                        purchase_status = "è³¼å…¥æ¸ˆã¿ or ç„¡æ–™" if price_element else "ç„¡æ–™"
                        
                        articles.append({
                            'ç•ªå·': i,
                            'å…¬é–‹æ—¥': date,
                            'ã‚¿ã‚¤ãƒˆãƒ«': title,
                            'æœ¬æ–‡': body_text,
                            'ä¾¡æ ¼': price,
                            'è³¼å…¥çŠ¶æ³': purchase_status
                        })
                        
                        print(f"âœ… '{title[:50]}...' ã‚’å–å¾—å®Œäº†")
                        
                    except Exception as e:
                        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                    
                    await asyncio.sleep(1.5)
                
                # CSVä¿å­˜
                timestamp = datetime.now().strftime("%Y%m%d_%H%M")
                filename = f"ihayato_extracted_{timestamp}.csv"
                
                df = pd.DataFrame(articles)
                df.to_csv(filename, index=False, encoding='utf-8-sig')
                
                print(f"\nğŸ‰ å®Œäº†! {len(articles)} è¨˜äº‹ã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸ")
                
        await browser.close()

if __name__ == "__main__":
    asyncio.run(main())