"""
å¢—åˆ†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
æ–°è¦è¨˜äº‹ã®ã¿ã‚’åŠ¹ç‡çš„ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
"""

import asyncio
from typing import List, Dict, Set
from bs4 import BeautifulSoup

from .browser import BrowserManager
from .collector import ArticleCollector
from .formatter import ContentFormatter


class IncrementalScraper:
    """æ–°è¦è¨˜äº‹ã®ã¿ã‚’åŠ¹ç‡çš„ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, headless: bool = False):
        self.browser_manager = BrowserManager(headless)
        self.collector = ArticleCollector()
        self.formatter = ContentFormatter()
        
    async def scrape_new_articles_only(self, new_urls: List[str]) -> List[Dict]:
        """æ–°è¦è¨˜äº‹URLã®ã¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        if not new_urls:
            print("ğŸ“ æ–°è¦è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“")
            return []
        
        print(f"ğŸš€ æ–°è¦è¨˜äº‹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {len(new_urls)}ä»¶")
        
        try:
            # ãƒ–ãƒ©ã‚¦ã‚¶åˆæœŸåŒ–
            await self.browser_manager.initialize()
            
            articles = []
            for i, url in enumerate(new_urls, 1):
                print(f"ğŸ“„ æ–°è¦è¨˜äº‹ {i}/{len(new_urls)}: {url}")
                
                try:
                    article = await self._scrape_single_article(url)
                    articles.append(article)
                    
                    # é€²æ—è¡¨ç¤º
                    if article.get('title'):
                        print(f"âœ… '{article['title'][:50]}...' ã‚’å–å¾—å®Œäº†")
                    else:
                        print(f"âœ… è¨˜äº‹å–å¾—å®Œäº†ï¼ˆã‚¿ã‚¤ãƒˆãƒ«å–å¾—å¤±æ•—ï¼‰")
                    
                    # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
                    await asyncio.sleep(1.5)
                    
                except Exception as e:
                    print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {url} - {e}")
                    # ã‚¨ãƒ©ãƒ¼ã§ã‚‚ç©ºã®ãƒ‡ãƒ¼ã‚¿ã§ç¶™ç¶š
                    articles.append({
                        'url': url,
                        'title': f"ã‚¨ãƒ©ãƒ¼: {e}",
                        'content': f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}",
                        'date': '',
                        'price': '',
                        'purchase_status': ''
                    })
            
            print(f"ğŸ‰ æ–°è¦è¨˜äº‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(articles)}ä»¶")
            return articles
            
        finally:
            await self.browser_manager.close()
    
    async def _scrape_single_article(self, url: str) -> Dict:
        """å˜ä¸€è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        # ãƒšãƒ¼ã‚¸ã«ç§»å‹•
        await self.browser_manager.navigate_to_article(url)
        
        # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        page_title = await self.browser_manager.get_page_title()
        title = ''
        if page_title:
            # noteã®æ§˜ã€…ãªã‚¿ã‚¤ãƒˆãƒ«å½¢å¼ã«å¯¾å¿œ
            if 'ï½œã‚¤ã‚±ãƒãƒ¤' in page_title:
                title = page_title.split('ï½œã‚¤ã‚±ãƒãƒ¤')[0].strip()
            elif 'ï½œnote' in page_title:
                title = page_title.split('ï½œnote')[0].strip()
            elif '|note' in page_title:
                title = page_title.split('|note')[0].strip()
            elif ' - note' in page_title:
                title = page_title.split(' - note')[0].strip()
            else:
                title = page_title.strip()
        
        # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’å–å¾—ã—ã¦ãƒ‘ãƒ¼ã‚¹
        content = await self.browser_manager.get_page_content()
        soup = BeautifulSoup(content, 'html.parser')
        
        # æœ¬æ–‡ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
        formatted_content = self.formatter.extract_formatted_content(soup)
        metadata = self.collector.extract_article_metadata(soup)
        
        # ãƒ‡ãƒãƒƒã‚°: ãƒãƒŠãƒ¼æ¤œå‡ºçŠ¶æ³
        if '[ãƒãƒŠãƒ¼:' in formatted_content or '[ç”»åƒãƒãƒŠãƒ¼:' in formatted_content:
            print(f"ğŸ” ãƒãƒŠãƒ¼æ¤œå‡º: {url}")
        
        # è¨˜äº‹æƒ…å ±ã‚’ä½œæˆ
        return {
            'url': url,
            'title': title,
            'content': formatted_content,
            'date': metadata['date'],
            'price': metadata['price'],
            'purchase_status': metadata['purchase_status']
        }
    
    async def quick_validate_urls(self, urls: List[str]) -> List[str]:
        """URLã®æœ‰åŠ¹æ€§ã‚’ç´ æ—©ããƒã‚§ãƒƒã‚¯"""
        print(f"ğŸ” URLæœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯é–‹å§‹: {len(urls)}ä»¶")
        
        valid_urls = []
        
        try:
            await self.browser_manager.initialize()
            
            for i, url in enumerate(urls):
                try:
                    # è»½é‡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿å–å¾—ï¼‰
                    await self.browser_manager.navigate_to_article(url)
                    title = await self.browser_manager.get_page_title()
                    
                    if title and ('note' in title.lower() or len(title) > 5):
                        valid_urls.append(url)
                    else:
                        print(f"âš ï¸  ç„¡åŠ¹ãªãƒšãƒ¼ã‚¸: {url}")
                    
                    # é«˜é€Ÿãƒã‚§ãƒƒã‚¯ã®ãŸã‚çŸ­ã„é–“éš”
                    await asyncio.sleep(0.5)
                    
                except Exception as e:
                    print(f"âŒ URLãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {url} - {e}")
                    continue
        
        finally:
            await self.browser_manager.close()
        
        print(f"âœ… URLæœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯å®Œäº†: {len(urls)}ä»¶ â†’ {len(valid_urls)}ä»¶ï¼ˆæœ‰åŠ¹ï¼‰")
        return valid_urls
    
    async def get_all_article_urls_from_page(self, profile_url: str, 
                                           manual_setup: bool = True) -> List[str]:
        """è¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨è¨˜äº‹URLã‚’å–å¾—ï¼ˆå¤–éƒ¨ã§ãƒ–ãƒ©ã‚¦ã‚¶ç®¡ç†ã•ã‚Œã‚‹å ´åˆã‚‚å¯¾å¿œï¼‰"""
        print(f"ğŸŒ è¨˜äº‹ä¸€è¦§ã‹ã‚‰URLå–å¾—é–‹å§‹: {profile_url}")
        
        # å¤–éƒ¨ã§ãƒ–ãƒ©ã‚¦ã‚¶ãŒæ—¢ã«åˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
        browser_initialized_externally = self.browser_manager.page is not None
        
        try:
            if not browser_initialized_externally:
                await self.browser_manager.initialize()
            
            # è¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸ã«ç§»å‹•
            article_list_url = await self.browser_manager.navigate_to_article_list(profile_url)
            print(f"ğŸ“„ è¨˜äº‹ä¸€è¦§ã«ç§»å‹•: {article_list_url}")
            
            if manual_setup:
                await self._wait_for_manual_setup()
            
            # è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’åé›†
            article_urls = await self.collector.collect_article_links(self.browser_manager.page)
            
            print(f"âœ… è¨˜äº‹URLå–å¾—å®Œäº†: {len(article_urls)}ä»¶")
            return article_urls
            
        finally:
            # å¤–éƒ¨ã§ãƒ–ãƒ©ã‚¦ã‚¶ãŒç®¡ç†ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯é–‰ã˜ãªã„
            if not browser_initialized_externally:
                await self.browser_manager.close()
    
    async def _wait_for_manual_setup(self):
        """æ‰‹å‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¾…æ©Ÿ"""
        import os
        
        print("\n" + "="*70)
        print("ğŸ”§ è¨˜äº‹ä¸€è¦§ã®æº–å‚™")
        print("="*70)
        print("ğŸ“‹ æ‰‹é †:")
        print("1. ğŸ“ ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰")
        print("2. ğŸ”„ ã€Œã‚‚ã£ã¨ã¿ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å…¨è¨˜äº‹ã‚’è¡¨ç¤ºã—ã¦ãã ã•ã„")
        print("3. âœ… å®Œäº†ã—ãŸã‚‰ setup_done.txt ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„")
        print()
        
        setup_file = "/Users/yusukeohata/Desktop/development/note-scraper/setup_done.txt"
        
        # æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°å‰Šé™¤
        if os.path.exists(setup_file):
            os.remove(setup_file)
            print("ğŸ—‘ï¸  å‰å›ã®setup_done.txtãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        
        print("ğŸ‘† å®Œäº†ã—ãŸã‚‰ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print(f"   touch {setup_file}")
        print()
        print("â° setup_done.txt ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã‚’å¾…æ©Ÿä¸­...")
        print("   â€» ã“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹é–“ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¯é–‹å§‹ã•ã‚Œã¾ã›ã‚“")
        
        # ã‚ˆã‚Šç¢ºå®Ÿãªç›£è¦–ãƒ«ãƒ¼ãƒ—
        wait_count = 0
        while True:
            wait_count += 1
            if wait_count % 10 == 0:  # 30ç§’ã”ã¨ã«é€²æ—ã‚’è¡¨ç¤º
                print(f"â° å¾…æ©Ÿä¸­... ({wait_count * 3}ç§’çµŒé)")
            
            if os.path.exists(setup_file):
                print("âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ã‚’ç¢ºèªã—ã¾ã—ãŸ")
                try:
                    os.remove(setup_file)
                    print("âœ… setup_done.txt ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                except Exception as e:
                    print(f"âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
                break
            
            await asyncio.sleep(3)
        
        print("âœ… æ‰‹å‹•æº–å‚™å®Œäº†ï¼URLåé›†ã‚’é–‹å§‹ã—ã¾ã™")
        print("ğŸ”„ è¨˜äº‹åé›†ãƒ•ã‚§ãƒ¼ã‚ºã«ç§»è¡Œã—ã¾ã™...")
    
    async def batch_scrape_with_progress(self, urls: List[str], batch_size: int = 5) -> List[Dict]:
        """ãƒãƒƒãƒå‡¦ç†ã§é€²æ—è¡¨ç¤ºä»˜ãã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        if not urls:
            return []
        
        print(f"ğŸ“¦ ãƒãƒƒãƒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {len(urls)}ä»¶ ({batch_size}ä»¶/ãƒãƒƒãƒ)")
        
        try:
            await self.browser_manager.initialize()
            
            all_articles = []
            
            # ãƒãƒƒãƒã«åˆ†å‰²
            for batch_num in range(0, len(urls), batch_size):
                batch_urls = urls[batch_num:batch_num + batch_size]
                batch_index = batch_num // batch_size + 1
                total_batches = (len(urls) + batch_size - 1) // batch_size
                
                print(f"\nğŸ“¦ ãƒãƒƒãƒ {batch_index}/{total_batches} å‡¦ç†ä¸­ ({len(batch_urls)}ä»¶)")
                
                batch_articles = []
                for i, url in enumerate(batch_urls):
                    global_index = batch_num + i + 1
                    print(f"ğŸ“„ è¨˜äº‹ {global_index}/{len(urls)}: {url}")
                    
                    try:
                        article = await self._scrape_single_article(url)
                        batch_articles.append(article)
                        
                        if article.get('title'):
                            print(f"âœ… '{article['title'][:50]}...' ã‚’å–å¾—å®Œäº†")
                        
                        await asyncio.sleep(1.5)
                        
                    except Exception as e:
                        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                        batch_articles.append({
                            'url': url,
                            'title': f"ã‚¨ãƒ©ãƒ¼: {e}",
                            'content': f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}",
                            'date': '',
                            'price': '',
                            'purchase_status': ''
                        })
                
                all_articles.extend(batch_articles)
                print(f"âœ… ãƒãƒƒãƒ {batch_index} å®Œäº† ({len(batch_articles)}ä»¶)")
            
            print(f"ğŸ‰ å…¨ãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(all_articles)}ä»¶")
            return all_articles
            
        finally:
            await self.browser_manager.close()