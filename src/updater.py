"""
ãƒ¡ã‚¤ãƒ³æ›´æ–°ãƒ­ã‚¸ãƒƒã‚¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ã™ã¹ã¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµ±åˆã—ã¦å¢—åˆ†æ›´æ–°ã‚’å®Ÿè¡Œ
"""

import asyncio
from typing import List, Dict, Optional
from pathlib import Path

from .csv_manager import CSVManager
from .url_differ import URLDiffer
from .incremental_scraper import IncrementalScraper


class NoteScrapeUpdater:
    """Noteè¨˜äº‹ã®å¢—åˆ†æ›´æ–°ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, profile_url: str, headless: bool = False):
        self.profile_url = profile_url
        self.csv_manager = CSVManager()
        self.url_differ = URLDiffer()
        self.scraper = IncrementalScraper(headless)
    
    async def update_from_csv(self, existing_csv_path: str, 
                             manual_setup: bool = True,
                             output_path: Optional[str] = None) -> Dict[str, any]:
        """æ—¢å­˜CSVã‹ã‚‰å¢—åˆ†æ›´æ–°ã‚’å®Ÿè¡Œ"""
        
        print("ğŸš€ Noteè¨˜äº‹ã®å¢—åˆ†æ›´æ–°ã‚’é–‹å§‹ã—ã¾ã™")
        print(f"ğŸ“„ å¯¾è±¡ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«: {self.profile_url}")
        print(f"ğŸ“ æ—¢å­˜CSV: {existing_csv_path}")
        print("=" * 70)
        
        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: æ—¢å­˜CSVã®èª­ã¿è¾¼ã¿
            print("ğŸ“‚ ã‚¹ãƒ†ãƒƒãƒ—1: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿")
            existing_data = self.csv_manager.load_existing_csv(existing_csv_path)
            existing_urls = existing_data['stats']['existing_urls']
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: ç¾åœ¨ã®è¨˜äº‹ä¸€è¦§ã‚’å–å¾—
            print("\nğŸŒ ã‚¹ãƒ†ãƒƒãƒ—2: ç¾åœ¨ã®è¨˜äº‹ä¸€è¦§ã‚’å–å¾—")
            current_urls = await self.scraper.get_all_article_urls_from_page(
                self.profile_url, manual_setup=manual_setup
            )
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: æ–°è¦URLã‚’è¨ˆç®—
            print("\nğŸ” ã‚¹ãƒ†ãƒƒãƒ—3: æ–°è¦è¨˜äº‹ã®ç‰¹å®š")
            new_urls = self.url_differ.calculate_new_urls(existing_urls, current_urls)
            
            # ã‚¹ãƒ†ãƒƒãƒ—4: æ–°è¦è¨˜äº‹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            print(f"\nğŸ“ ã‚¹ãƒ†ãƒƒãƒ—4: æ–°è¦è¨˜äº‹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°")
            if new_urls:
                new_articles = await self.scraper.scrape_new_articles_only(new_urls)
            else:
                print("ğŸ“ æ–°è¦è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“")
                new_articles = []
            
            # ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ¼ã‚¸ã¨ä¿å­˜
            print(f"\nğŸ’¾ ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ¼ã‚¸ã¨ä¿å­˜")
            result = self.csv_manager.merge_and_save(
                existing_csv_path, new_articles, output_path
            )
            
            # çµæœã‚µãƒãƒªãƒ¼
            print("\n" + "=" * 70)
            print("ğŸ‰ æ›´æ–°å®Œäº†!")
            print(f"ğŸ“Š æ—¢å­˜è¨˜äº‹: {len(existing_urls)}ä»¶")
            print(f"ğŸ“Š ç¾åœ¨è¨˜äº‹: {len(current_urls)}ä»¶")
            print(f"ğŸ“Š æ–°è¦è¨˜äº‹: {len(new_urls)}ä»¶")
            print(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {result['filename']}")
            print(f"ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {result['file_size_mb']} MB")
            print("=" * 70)
            
            return {
                'success': True,
                'existing_count': len(existing_urls),
                'current_count': len(current_urls),
                'new_count': len(new_urls),
                'new_articles': new_articles,
                'output_file': result['filename'],
                'file_size_mb': result['file_size_mb']
            }
            
        except Exception as e:
            print(f"âŒ æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def update_with_validation(self, existing_csv_path: str,
                                   manual_setup: bool = True,
                                   validate_urls: bool = True,
                                   batch_size: int = 5,
                                   output_path: Optional[str] = None) -> Dict[str, any]:
        """URLæ¤œè¨¼ä»˜ãã®å¢—åˆ†æ›´æ–°"""
        
        print("ğŸš€ URLæ¤œè¨¼ä»˜ãå¢—åˆ†æ›´æ–°ã‚’é–‹å§‹ã—ã¾ã™")
        
        try:
            # åŸºæœ¬æ›´æ–°ã‚’å®Ÿè¡Œ
            update_result = await self.update_from_csv(
                existing_csv_path, manual_setup, output_path
            )
            
            if not update_result['success']:
                return update_result
            
            new_urls = [article['url'] for article in update_result['new_articles']]
            
            # URLæ¤œè¨¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if validate_urls and new_urls:
                print(f"\nğŸ” ã‚¹ãƒ†ãƒƒãƒ—6: æ–°è¦URLã®æ¤œè¨¼")
                valid_urls = await self.scraper.quick_validate_urls(new_urls)
                invalid_count = len(new_urls) - len(valid_urls)
                
                if invalid_count > 0:
                    print(f"âš ï¸  ç„¡åŠ¹ãªURL: {invalid_count}ä»¶")
                    update_result['invalid_urls_count'] = invalid_count
            
            return update_result
            
        except Exception as e:
            print(f"âŒ æ¤œè¨¼ä»˜ãæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def batch_update_with_progress(self, existing_csv_path: str,
                                       manual_setup: bool = True,
                                       batch_size: int = 5,
                                       output_path: Optional[str] = None) -> Dict[str, any]:
        """ãƒãƒƒãƒå‡¦ç†ä»˜ãã®å¢—åˆ†æ›´æ–°ï¼ˆå˜ä¸€ãƒ–ãƒ©ã‚¦ã‚¶ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰"""
        
        print("ğŸš€ ãƒãƒƒãƒå‡¦ç†ä»˜ãå¢—åˆ†æ›´æ–°ã‚’é–‹å§‹ã—ã¾ã™")
        
        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
            existing_data = self.csv_manager.load_existing_csv(existing_csv_path)
            existing_urls = existing_data['stats']['existing_urls']
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: å˜ä¸€ãƒ–ãƒ©ã‚¦ã‚¶ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§å…¨å‡¦ç†ã‚’å®Ÿè¡Œ
            await self.scraper.browser_manager.initialize()
            
            try:
                # è¨˜äº‹ä¸€è¦§ã‹ã‚‰URLå–å¾—
                current_urls = await self.scraper.get_all_article_urls_from_page(
                    self.profile_url, manual_setup=manual_setup
                )
                
                # æ–°è¦URLè¨ˆç®—
                new_urls = self.url_differ.calculate_new_urls(existing_urls, current_urls)
                
                # æ–°è¦è¨˜äº‹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆåŒä¸€ãƒ–ãƒ©ã‚¦ã‚¶ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã§å®Ÿè¡Œï¼‰
                if new_urls:
                    print(f"\nğŸ“ æ–°è¦è¨˜äº‹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {len(new_urls)}ä»¶")
                    new_articles = []
                    
                    # ãƒãƒƒãƒå‡¦ç†ã§æ–°è¦è¨˜äº‹ã‚’å–å¾—
                    for batch_num in range(0, len(new_urls), batch_size):
                        batch_urls = new_urls[batch_num:batch_num + batch_size]
                        batch_index = batch_num // batch_size + 1
                        total_batches = (len(new_urls) + batch_size - 1) // batch_size
                        
                        print(f"\nğŸ“¦ ãƒãƒƒãƒ {batch_index}/{total_batches} å‡¦ç†ä¸­ ({len(batch_urls)}ä»¶)")
                        
                        for i, url in enumerate(batch_urls):
                            global_index = batch_num + i + 1
                            print(f"ğŸ“„ è¨˜äº‹ {global_index}/{len(new_urls)}: {url}")
                            
                            try:
                                article = await self.scraper._scrape_single_article(url)
                                new_articles.append(article)
                                
                                if article.get('title'):
                                    print(f"âœ… '{article['title'][:50]}...' ã‚’å–å¾—å®Œäº†")
                                
                                await asyncio.sleep(1.5)
                                
                            except Exception as e:
                                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                                new_articles.append({
                                    'url': url,
                                    'title': f"ã‚¨ãƒ©ãƒ¼: {e}",
                                    'content': f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}",
                                    'date': '',
                                    'price': '',
                                    'purchase_status': ''
                                })
                        
                        print(f"âœ… ãƒãƒƒãƒ {batch_index} å®Œäº†")
                    
                    print(f"ğŸ‰ å…¨ãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(new_articles)}ä»¶")
                else:
                    print("ğŸ“ æ–°è¦è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“")
                    new_articles = []
                
            finally:
                # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’ç¢ºå®Ÿã«é–‰ã˜ã‚‹
                await self.scraper.browser_manager.close()
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒãƒ¼ã‚¸ã¨ä¿å­˜
            result = self.csv_manager.merge_and_save(
                existing_csv_path, new_articles, output_path
            )
            
            print(f"\nğŸ‰ ãƒãƒƒãƒæ›´æ–°å®Œäº†! æ–°è¦è¨˜äº‹: {len(new_articles)}ä»¶")
            
            return {
                'success': True,
                'existing_count': len(existing_urls),
                'current_count': len(current_urls),
                'new_count': len(new_urls),
                'new_articles': new_articles,
                'output_file': result['filename'],
                'file_size_mb': result['file_size_mb']
            }
            
        except Exception as e:
            print(f"âŒ ãƒãƒƒãƒæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def check_csv_compatibility(self, csv_path: str) -> bool:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã®äº’æ›æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            return self.csv_manager.validate_csv_format(csv_path)
        except Exception as e:
            print(f"âŒ CSVäº’æ›æ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def get_current_article_count(self, manual_setup: bool = True) -> int:
        """ç¾åœ¨ã®è¨˜äº‹æ•°ã‚’å–å¾—ï¼ˆæ›´æ–°å‰ã®ç¢ºèªç”¨ï¼‰"""
        try:
            current_urls = await self.scraper.get_all_article_urls_from_page(
                self.profile_url, manual_setup=manual_setup
            )
            return len(current_urls)
        except Exception as e:
            print(f"âŒ è¨˜äº‹æ•°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0
    
    def analyze_update_potential(self, existing_csv_path: str) -> Dict[str, any]:
        """æ›´æ–°ã®å¿…è¦æ€§ã‚’åˆ†æï¼ˆå®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‰ã®äº‹å‰ãƒã‚§ãƒƒã‚¯ï¼‰"""
        try:
            existing_data = self.csv_manager.load_existing_csv(existing_csv_path)
            stats = existing_data['stats']
            
            analysis = {
                'existing_articles': stats['total_articles'],
                'latest_date': stats['latest_date'],
                'date_range': stats['date_range'],
                'has_url_column': stats.get('has_url_column', True),
                'ready_for_update': True,
                'recommendations': []
            }
            
            # æ¨å¥¨äº‹é …
            if stats['total_articles'] < 10:
                analysis['recommendations'].append("æ—¢å­˜è¨˜äº‹æ•°ãŒå°‘ãªã„ãŸã‚ã€å…¨ä»¶å†å–å¾—ã‚‚æ¤œè¨ã—ã¦ãã ã•ã„")
            
            if stats['latest_date']:
                analysis['recommendations'].append(f"æœ€æ–°è¨˜äº‹æ—¥ä»˜: {stats['latest_date']}")
            
            return analysis
            
        except Exception as e:
            return {
                'ready_for_update': False,
                'error': str(e)
            }