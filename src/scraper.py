"""
ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

import asyncio
import os
from typing import List, Dict
from bs4 import BeautifulSoup
from datetime import datetime

from .browser import BrowserManager
from .collector import ArticleCollector
from .formatter import ContentFormatter
from .exporter import CSVExporter


class NoteScraper:
    """Noteã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, headless: bool = False):
        self.browser_manager = BrowserManager(headless)
        self.collector = ArticleCollector()
        self.formatter = ContentFormatter()
        self.exporter = CSVExporter()
        
    async def run(self, profile_url: str) -> Dict[str, any]:
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        try:
            print("ğŸš€ Note Scraper ã‚’é–‹å§‹")
            print(f"ğŸ“ å¯¾è±¡: {profile_url}")
            
            # ãƒ–ãƒ©ã‚¦ã‚¶åˆæœŸåŒ–
            await self.browser_manager.initialize()
            print("âœ… ãƒ–ãƒ©ã‚¦ã‚¶åˆæœŸåŒ–å®Œäº†")
            
            # æ‰‹å‹•æº–å‚™ãƒ•ã‚§ãƒ¼ã‚º
            await self._manual_setup_phase(profile_url)
            
            # è¨˜äº‹åé›†
            article_urls = await self.collector.collect_article_links(self.browser_manager.page)
            print(f"âœ… {len(article_urls)} è¨˜äº‹ã‚’ç™ºè¦‹")
            
            if not article_urls:
                print("âŒ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return {'success': False, 'error': 'No articles found'}
            
            # å„è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            articles = await self._scrape_articles(article_urls)
            
            # CSVä¿å­˜
            timestamp = datetime.now().strftime("%Y%m%d_%H%M")
            filename = f"ihayato_final_{timestamp}.csv"
            result = self.exporter.save_to_csv(articles, filename)
            
            print(f"ğŸ‰ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†!")
            print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {result['filename']}")
            print(f"ğŸ“Š è¨˜äº‹æ•°: {result['article_count']}")
            print(f"ğŸ’¾ ã‚µã‚¤ã‚º: {result['file_size_mb']} MB")
            
            return {
                'success': True,
                'filename': result['filename'],
                'article_count': result['article_count'],
                'file_size_mb': result['file_size_mb']
            }
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return {'success': False, 'error': str(e)}
        finally:
            await self.browser_manager.close()
    
    async def _manual_setup_phase(self, profile_url: str):
        """æ‰‹å‹•æº–å‚™ãƒ•ã‚§ãƒ¼ã‚º"""
        print("\n" + "="*70)
        print("ğŸ”§ æ‰‹å‹•æº–å‚™ãƒ•ã‚§ãƒ¼ã‚ºé–‹å§‹")
        print("="*70)
        
        # è¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸ã«ç§»å‹•
        article_list_url = await self.browser_manager.navigate_to_article_list(profile_url)
        print(f"ğŸ“„ è¨˜äº‹ä¸€è¦§ã«ç§»å‹•: {article_list_url}")
        
        print("\nğŸ“‹ æ‰‹å‹•ä½œæ¥­ã®æ‰‹é †:")
        print("1. ğŸ“ ãƒ–ãƒ©ã‚¦ã‚¶ã§ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„")
        print("2. ğŸ”„ ã€Œã‚‚ã£ã¨ã¿ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’å…¨éƒ¨ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„")
        print("   ï¼ˆ600è¨˜äº‹ä»¥ä¸ŠãŒå…¨ã¦è¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§ï¼‰")
        print("3. âœ… å®Œäº†ã—ãŸã‚‰ setup_done.txt ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„")
        print()
        print("âš ï¸  é‡è¦:")
        print("   - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¯è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("   - ãƒ–ãƒ©ã‚¦ã‚¶ã¯è‡ªå‹•ã§é–‰ã˜ã¾ã›ã‚“")
        print("   - æ™‚é–“ã‚’ã‹ã‘ã¦ã‚‚å¤§ä¸ˆå¤«ã§ã™")
        print()
        
        # å®Œäº†ã¾ã§å¾…æ©Ÿ
        setup_file = "/Users/yusukeohata/Desktop/development/note-scraper/setup_done.txt"
        print("ğŸ‘† å®Œäº†ã—ãŸã‚‰ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print(f"   touch {setup_file}")
        print()
        print("â° setup_done.txt ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã‚’å¾…æ©Ÿä¸­...")
        
        while True:
            if os.path.exists(setup_file):
                print("âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ã‚’ç¢ºèªã—ã¾ã—ãŸ")
                os.remove(setup_file)
                break
            await asyncio.sleep(3)
        
        print("âœ… æ‰‹å‹•æº–å‚™å®Œäº†ï¼è‡ªå‹•å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
    
    async def _scrape_articles(self, article_urls: List[str]) -> List[Dict]:
        """è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        print(f"\nğŸ“„ {len(article_urls)} è¨˜äº‹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹...")
        
        articles = []
        for i, url in enumerate(article_urls, 1):
            try:
                print(f"ğŸ“„ è¨˜äº‹ {i}/{len(article_urls)}: {url}")
                
                # ãƒšãƒ¼ã‚¸ã«ç§»å‹•
                await self.browser_manager.navigate_to_article(url)
                
                # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                page_title = await self.browser_manager.get_page_title()
                title = ''
                if page_title and 'ï½œnote' in page_title:
                    title = page_title.split('ï½œnote')[0].strip()
                
                # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’å–å¾—ã—ã¦ãƒ‘ãƒ¼ã‚¹
                content = await self.browser_manager.get_page_content()
                soup = BeautifulSoup(content, 'html.parser')
                
                # æœ¬æ–‡ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
                formatted_content = self.formatter.extract_formatted_content(soup)
                metadata = self.collector.extract_article_metadata(soup)
                
                # è¨˜äº‹æƒ…å ±ã‚’ä½œæˆ
                article = {
                    'url': url,
                    'title': title,
                    'content': formatted_content,
                    'date': metadata['date'],
                    'price': metadata['price'],
                    'purchase_status': metadata['purchase_status']
                }
                
                articles.append(article)
                print(f"âœ… '{title[:50]}...' ã‚’å–å¾—å®Œäº†")
                
                # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
                await asyncio.sleep(1.5)
                
            except Exception as e:
                print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {url} - {e}")
                # ã‚¨ãƒ©ãƒ¼ã§ã‚‚å‡¦ç†ç¶™ç¶š
                articles.append({
                    'url': url,
                    'title': f"ã‚¨ãƒ©ãƒ¼: {e}",
                    'content': f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}",
                    'date': '',
                    'price': '',
                    'purchase_status': ''
                })
        
        return articles