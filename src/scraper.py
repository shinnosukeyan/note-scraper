"""
ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

import asyncio
import os
from typing import List, Dict
from bs4 import BeautifulSoup
from datetime import datetime

from .browser import BrowserManager
from .collector import ArticleCollector
from .formatter import ContentFormatter
from .exporter import CSVExporter


class NoteScraper:
    """Noteã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, headless: bool = False):
        self.browser_manager = BrowserManager(headless)
        self.collector = ArticleCollector()
        self.formatter = ContentFormatter()
        self.exporter = CSVExporter()
        
    async def run(self, profile_url: str) -> Dict[str, any]:
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        try:
            print("ğŸš€ Note Scraper ã‚’é–‹å§‹")
            print(f"ğŸ“ å¯¾è±¡: {profile_url}")
            
            # ãƒ–ãƒ©ã‚¦ã‚¶åˆæœŸåŒ–
            await self.browser_manager.initialize()
            print("âœ… ãƒ–ãƒ©ã‚¦ã‚¶åˆæœŸåŒ–å®Œäº†")
            
            # æ‰‹å‹•æº–å‚™ãƒ•ã‚§ãƒ¼ã‚º
            await self._manual_setup_phase(profile_url)
            
            # è¨˜äº‹åé›†
            article_urls = await self.collector.collect_article_links(self.browser_manager.page)
            print(f"âœ… {len(article_urls)} è¨˜äº‹ã‚’ç™ºè¦‹")
            
            # ãƒ‡ãƒãƒƒã‚°: è¨˜äº‹æ•°ãŒå°‘ãªã„å ´åˆã®è©³ç´°æƒ…å ±
            if len(article_urls) < 30:
                print(f"âš ï¸  è¨˜äº‹æ•°ãŒå°‘ãªã„ã§ã™ ({len(article_urls)}è¨˜äº‹)")
                print("ğŸ” ãƒšãƒ¼ã‚¸å†…ã®å…¨ãƒªãƒ³ã‚¯ã‚’èª¿æŸ»ä¸­...")
                
                # å…¨ãƒªãƒ³ã‚¯ã‚’ãƒ‡ãƒãƒƒã‚°
                all_page_links = await self.browser_manager.page.query_selector_all('a')
                note_links = []
                for link in all_page_links:
                    try:
                        href = await link.get_attribute('href')
                        if href and '/n/' in href:
                            note_links.append(href)
                    except:
                        continue
                        
                print(f"ğŸ” ãƒšãƒ¼ã‚¸å†…ã®/n/ãƒªãƒ³ã‚¯ç·æ•°: {len(note_links)}")
                print(f"ğŸ” æœ€åˆã®10å€‹: {note_links[:10]}")
                
                # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«çŠ¶æ³ç¢ºèª
                scroll_height = await self.browser_manager.page.evaluate('document.body.scrollHeight')
                print(f"ğŸ” ãƒšãƒ¼ã‚¸ã®é«˜ã•: {scroll_height}px")
                
                # ã‚‚ã£ã¨ã¿ã‚‹ãƒœã‚¿ãƒ³ã®å­˜åœ¨ç¢ºèª
                more_buttons = await self.browser_manager.page.query_selector_all('button')
                for button in more_buttons:
                    try:
                        text = await button.text_content()
                        if text and 'ã‚‚ã£ã¨' in text:
                            print(f"ğŸ” ç™ºè¦‹ã—ãŸãƒœã‚¿ãƒ³: '{text}'")
                    except:
                        continue
            
            if not article_urls:
                print("âŒ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return {'success': False, 'error': 'No articles found'}
            
            # å„è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            articles = await self._scrape_articles(article_urls)
            
            # CSVä¿å­˜
            timestamp = datetime.now().strftime("%Y%m%d_%H%M")
            filename = f"ihayato_final_{timestamp}.csv"
            result = self.exporter.save_to_csv(articles, filename)
            
            print(f"ğŸ‰ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†!")
            print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {result['filename']}")
            print(f"ğŸ“Š è¨˜äº‹æ•°: {result['article_count']}")
            print(f"ğŸ’¾ ã‚µã‚¤ã‚º: {result['file_size_mb']} MB")
            
            return {
                'success': True,
                'filename': result['filename'],
                'article_count': result['article_count'],
                'file_size_mb': result['file_size_mb']
            }
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return {'success': False, 'error': str(e)}
        finally:
            await self.browser_manager.close()
    
    async def _manual_setup_phase(self, profile_url: str):
        """æ‰‹å‹•æº–å‚™ãƒ•ã‚§ãƒ¼ã‚º"""
        print("\n" + "="*70)
        print("ğŸ”§ æ‰‹å‹•æº–å‚™ãƒ•ã‚§ãƒ¼ã‚ºé–‹å§‹")
        print("="*70)
        
        # è¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸ã«ç§»å‹•
        article_list_url = await self.browser_manager.navigate_to_article_list(profile_url)
        print(f"ğŸ“„ è¨˜äº‹ä¸€è¦§ã«ç§»å‹•: {article_list_url}")
        
        print("\nğŸ“‹ æ‰‹å‹•ä½œæ¥­ã®æ‰‹é †:")
        print("1. ğŸ“ ãƒ–ãƒ©ã‚¦ã‚¶ã§ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„")
        print("2. ğŸ”„ ã€Œã‚‚ã£ã¨ã¿ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’å…¨éƒ¨ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„")
        print("   ï¼ˆ600è¨˜äº‹ä»¥ä¸ŠãŒå…¨ã¦è¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§ï¼‰")
        print("3. âœ… å®Œäº†ã—ãŸã‚‰ setup_done.txt ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„")
        print()
        print("âš ï¸  é‡è¦:")
        print("   - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¯è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("   - ãƒ–ãƒ©ã‚¦ã‚¶ã¯è‡ªå‹•ã§é–‰ã˜ã¾ã›ã‚“")
        print("   - æ™‚é–“ã‚’ã‹ã‘ã¦ã‚‚å¤§ä¸ˆå¤«ã§ã™")
        print()
        
        # å®Œäº†ã¾ã§å¾…æ©Ÿ
        setup_file = "/Users/yusukeohata/Desktop/development/note-scraper/setup_done.txt"
        print("ğŸ‘† å®Œäº†ã—ãŸã‚‰ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print(f"   touch {setup_file}")
        print()
        print("â° setup_done.txt ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã‚’å¾…æ©Ÿä¸­...")
        
        while True:
            if os.path.exists(setup_file):
                print("âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ã‚’ç¢ºèªã—ã¾ã—ãŸ")
                os.remove(setup_file)
                break
            await asyncio.sleep(3)
        
        print("âœ… æ‰‹å‹•æº–å‚™å®Œäº†ï¼è‡ªå‹•å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
    
    async def _scrape_articles(self, article_urls: List[str]) -> List[Dict]:
        """è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        print(f"\nğŸ“„ {len(article_urls)} è¨˜äº‹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹...")
        
        articles = []
        for i, url in enumerate(article_urls, 1):
            try:
                print(f"ğŸ“„ è¨˜äº‹ {i}/{len(article_urls)}: {url}")
                
                # ãƒšãƒ¼ã‚¸ã«ç§»å‹•
                await self.browser_manager.navigate_to_article(url)
                
                # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                page_title = await self.browser_manager.get_page_title()
                title = ''
                if page_title and 'ï½œnote' in page_title:
                    title = page_title.split('ï½œnote')[0].strip()
                
                # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’å–å¾—ã—ã¦ãƒ‘ãƒ¼ã‚¹
                content = await self.browser_manager.get_page_content()
                soup = BeautifulSoup(content, 'html.parser')
                
                # æœ¬æ–‡ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
                formatted_content = self.formatter.extract_formatted_content(soup)
                metadata = self.collector.extract_article_metadata(soup)
                
                # ãƒ‡ãƒãƒƒã‚°: ãƒãƒŠãƒ¼æ¤œå‡ºçŠ¶æ³
                if '[ãƒãƒŠãƒ¼:' in formatted_content or '[ç”»åƒãƒãƒŠãƒ¼:' in formatted_content:
                    print(f"ğŸ” ãƒãƒŠãƒ¼æ¤œå‡º: {url}")
                
                # ãƒ‡ãƒãƒƒã‚°: åŸ‹ã‚è¾¼ã¿æ¤œå‡ºçŠ¶æ³  
                if '[åŸ‹ã‚è¾¼ã¿' in formatted_content or '[YouTube' in formatted_content or '[Twitter' in formatted_content:
                    print(f"ğŸ” åŸ‹ã‚è¾¼ã¿æ¤œå‡º: {url}")
                
                # è¨˜äº‹æƒ…å ±ã‚’ä½œæˆ
                article = {
                    'url': url,
                    'title': title,
                    'content': formatted_content,
                    'date': metadata['date'],
                    'price': metadata['price'],
                    'purchase_status': metadata['purchase_status']
                }
                
                articles.append(article)
                print(f"âœ… '{title[:50]}...' ã‚’å–å¾—å®Œäº†")
                
                # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
                await asyncio.sleep(1.5)
                
            except Exception as e:
                print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {url} - {e}")
                # ã‚¨ãƒ©ãƒ¼ã§ã‚‚å‡¦ç†ç¶™ç¶š
                articles.append({
                    'url': url,
                    'title': f"ã‚¨ãƒ©ãƒ¼: {e}",
                    'content': f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}",
                    'date': '',
                    'price': '',
                    'purchase_status': ''
                })
        
        return articles